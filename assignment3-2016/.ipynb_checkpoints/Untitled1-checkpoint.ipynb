{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "('Total addition questions:', 10100)\n",
      "Vectorization...\n",
      "Training Data:\n",
      "(9090, 22, 13)\n",
      "(9090, 22)\n",
      "Validation Data:\n",
      "(1010, 22, 13)\n",
      "(1010, 22)\n"
     ]
    }
   ],
   "source": [
    "# Some parts of this code are taken from\n",
    "# https://github.com/fchollet/keras/blob/master/examples/addition_rnn.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "np.random.seed(22) # that's my lucky number\n",
    "\n",
    "# a helper class to encode and decode sequences from one-hot encoding\n",
    "class CharacterTable(object):\n",
    "    def __init__(self, chars):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "    \n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 10000\n",
    "DIGITS = 20 # max input size that the model can accept\n",
    "MAXLEN = DIGITS + 2 # s(start symbol) + number + e(end symbol)\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789se '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "# This is the primary dataset\n",
    "# we will use numbers upto 10 digits only\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, 10))))\n",
    "    a = f()\n",
    "    b = a + 1\n",
    "    key = (a, b)\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    \n",
    "    q = 'e{}s'.format(a)\n",
    "    query = ' ' * (MAXLEN - len(q)) + q\n",
    "    ans = 'e{}s'.format(b)\n",
    "    ans = ' ' * (MAXLEN - len(ans)) + ans\n",
    "    \n",
    "    ans= ans[::-1]\n",
    "    query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "\n",
    "# There will be very few examples in the primary dataset which looks like 1239999999999999. If we use only primary\n",
    "# dataset then the rnn will fail on these type of examples. Because it fails to remember carry for this longer.\n",
    "# so here is a small dataset of 100 examples which contains numbers of the form ...99999999999.\n",
    "# If we also train on this one then the model works for all type of input\n",
    "# This is just a hack but it works!\n",
    "while len(questions) < TRAINING_SIZE + 100:\n",
    "    f = lambda: int(''.join(np.random.choice(list('123456789'))\n",
    "                    for i in range(np.random.randint(0,3))) + '9'*np.random.randint(1, 7))\n",
    "    a = f()\n",
    "    b = a + 1\n",
    "\n",
    "    key = (a, b)\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    \n",
    "    q = 'e{}s'.format(a)\n",
    "    query = ' ' * (MAXLEN - len(q)) + q\n",
    "    ans = 'e{}s'.format(b)\n",
    "    ans = ' ' * (MAXLEN - len(ans)) + ans\n",
    "    \n",
    "    ans= ans[::-1]\n",
    "    query = query[::-1]\n",
    "    \n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.int32)\n",
    "y = np.zeros((len(questions), MAXLEN), dtype=np.int32)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = np.array([ctable.char_indices[z] for z in sentence])\n",
    "\n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger digits\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = len(chars) #input dimension\n",
    "C = len(chars) #output dimension\n",
    "T = MAXLEN # truncated backprop length\n",
    "N = 5 #batch_size\n",
    "H = 50 #hidden dimension\n",
    "num_epochs = 5\n",
    "num_batches = x_train.shape[0]//N\n",
    "batch_size = N\n",
    "\n",
    "# build tensorflow model\n",
    "tf.reset_default_graph()\n",
    "\n",
    "x_batch = tf.placeholder(tf.float32, [None,T,D], name='x_batch')\n",
    "y_batch = tf.placeholder(tf.int64, [None,T], name='y_batch')\n",
    "\n",
    "N = tf.shape(x_batch)[0]\n",
    "cell_state = tf.zeros([N,H], dtype=tf.float32)\n",
    "hidden_state = tf.zeros([N,H], dtype=tf.float32)\n",
    "init_state = tf.contrib.rnn.LSTMStateTuple(cell_state, hidden_state)\n",
    "\n",
    "w_init = tf.random_normal([H, C], name='wc_init')\n",
    "b_init = tf.zeros([1, C], name='bc_init')\n",
    "W = tf.Variable(w_init, dtype=tf.float32, name='wc')\n",
    "b = tf.Variable(b_init, dtype=tf.float32, name='bc')\n",
    "\n",
    "cell = tf.contrib.rnn.BasicLSTMCell(H, state_is_tuple=True)\n",
    "states_series, current_state = tf.nn.dynamic_rnn(cell, x_batch, initial_state=init_state)\n",
    "states_series = tf.reshape(states_series, [-1, H])\n",
    "logits = tf.matmul(states_series, W) + b\n",
    "logits = tf.reshape(logits, [-1,T,C])\n",
    "losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_batch)\n",
    "prob = tf.nn.softmax(logits)\n",
    "pred = tf.argmax(prob, axis=-1, name='pred')\n",
    "acc = tf.contrib.metrics.accuracy(pred, y_batch)\n",
    "\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(1).minimize(total_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 0)\n",
      "('Step', 0, 'Batch loss', 2.5730286)\n",
      "('Step', 100, 'Batch loss', 0.14872751)\n",
      "('Step', 200, 'Batch loss', 0.075055964)\n",
      "('Step', 300, 'Batch loss', 0.039490622)\n",
      "('Step', 400, 'Batch loss', 0.022574011)\n",
      "('Step', 500, 'Batch loss', 0.0067824828)\n",
      "('Step', 600, 'Batch loss', 0.0044176304)\n",
      "('Step', 700, 'Batch loss', 0.0031987878)\n",
      "('Step', 800, 'Batch loss', 0.0022561862)\n",
      "('Step', 900, 'Batch loss', 0.020628434)\n",
      "('Step', 1000, 'Batch loss', 0.0031572352)\n",
      "('Step', 1100, 'Batch loss', 0.0020873295)\n",
      "('Step', 1200, 'Batch loss', 0.0014098067)\n",
      "('Step', 1300, 'Batch loss', 0.0013639374)\n",
      "('Step', 1400, 'Batch loss', 0.00092763995)\n",
      "('Step', 1500, 'Batch loss', 0.0010974328)\n",
      "('Step', 1600, 'Batch loss', 0.0018143603)\n",
      "('Step', 1700, 'Batch loss', 0.00091969763)\n",
      "('Step', 1800, 'Batch loss', 0.00062676193)\n",
      "('Training accuracy: ', 0.99999499)\n",
      "('Validation accuracy: ', 1.0)\n",
      "('Epoch', 1)\n",
      "('Step', 0, 'Batch loss', 0.00067146576)\n",
      "('Step', 100, 'Batch loss', 0.00075549498)\n",
      "('Step', 200, 'Batch loss', 0.00051925814)\n",
      "('Step', 300, 'Batch loss', 0.00052948337)\n",
      "('Step', 400, 'Batch loss', 0.0005810968)\n",
      "('Step', 500, 'Batch loss', 0.00035646625)\n",
      "('Step', 600, 'Batch loss', 0.00041741677)\n",
      "('Step', 700, 'Batch loss', 0.00035572049)\n",
      "('Step', 800, 'Batch loss', 0.0003158375)\n",
      "('Step', 900, 'Batch loss', 0.0004762976)\n",
      "('Step', 1000, 'Batch loss', 0.00029757686)\n",
      "('Step', 1100, 'Batch loss', 0.00026900898)\n",
      "('Step', 1200, 'Batch loss', 0.00025608123)\n",
      "('Step', 1300, 'Batch loss', 0.00029683913)\n",
      "('Step', 1400, 'Batch loss', 0.00022232468)\n",
      "('Step', 1500, 'Batch loss', 0.00029436045)\n",
      "('Step', 1600, 'Batch loss', 0.00045292615)\n",
      "('Step', 1700, 'Batch loss', 0.00029572012)\n",
      "('Step', 1800, 'Batch loss', 0.00021627601)\n",
      "('Training accuracy: ', 0.99999499)\n",
      "('Validation accuracy: ', 1.0)\n",
      "('Epoch', 2)\n",
      "('Step', 0, 'Batch loss', 0.00024278698)\n",
      "('Step', 100, 'Batch loss', 0.00026793833)\n",
      "('Step', 200, 'Batch loss', 0.0002039502)\n",
      "('Step', 300, 'Batch loss', 0.00022342136)\n",
      "('Step', 400, 'Batch loss', 0.00024480993)\n",
      "('Step', 500, 'Batch loss', 0.00016538479)\n",
      "('Step', 600, 'Batch loss', 0.00019980178)\n",
      "('Step', 700, 'Batch loss', 0.00017380754)\n",
      "('Step', 800, 'Batch loss', 0.00015969055)\n",
      "('Step', 900, 'Batch loss', 0.00023793215)\n",
      "('Step', 1000, 'Batch loss', 0.00015071913)\n",
      "('Step', 1100, 'Batch loss', 0.00014771716)\n",
      "('Step', 1200, 'Batch loss', 0.00014282083)\n",
      "('Step', 1300, 'Batch loss', 0.00016706061)\n",
      "('Step', 1400, 'Batch loss', 0.00012835898)\n",
      "('Step', 1500, 'Batch loss', 0.00016399266)\n",
      "('Step', 1600, 'Batch loss', 0.00024058425)\n",
      "('Step', 1700, 'Batch loss', 0.00017095015)\n",
      "('Step', 1800, 'Batch loss', 0.00012973668)\n",
      "('Training accuracy: ', 1.0)\n",
      "('Validation accuracy: ', 1.0)\n",
      "('Epoch', 3)\n",
      "('Step', 0, 'Batch loss', 0.00014927458)\n",
      "('Step', 100, 'Batch loss', 0.0001613615)\n",
      "('Step', 200, 'Batch loss', 0.00012563179)\n",
      "('Step', 300, 'Batch loss', 0.00013729713)\n",
      "('Step', 400, 'Batch loss', 0.00015280819)\n",
      "('Step', 500, 'Batch loss', 0.00010756382)\n",
      "('Step', 600, 'Batch loss', 0.00013055143)\n",
      "('Step', 700, 'Batch loss', 0.00011391244)\n",
      "('Step', 800, 'Batch loss', 0.00010620253)\n",
      "('Step', 900, 'Batch loss', 0.00015571623)\n",
      "('Step', 1000, 'Batch loss', 0.00010031026)\n",
      "('Step', 1100, 'Batch loss', 0.0001015349)\n",
      "('Step', 1200, 'Batch loss', 9.8204793e-05)\n",
      "('Step', 1300, 'Batch loss', 0.00011542916)\n",
      "('Step', 1400, 'Batch loss', 9.0025875e-05)\n",
      "('Step', 1500, 'Batch loss', 0.00011237235)\n",
      "('Step', 1600, 'Batch loss', 0.00016002032)\n",
      "('Step', 1700, 'Batch loss', 0.00011876121)\n",
      "('Step', 1800, 'Batch loss', 9.2050257e-05)\n",
      "('Training accuracy: ', 1.0)\n",
      "('Validation accuracy: ', 1.0)\n",
      "('Epoch', 4)\n",
      "('Step', 0, 'Batch loss', 0.00010708012)\n",
      "('Step', 100, 'Batch loss', 0.00011493715)\n",
      "('Step', 200, 'Batch loss', 9.0438618e-05)\n",
      "('Step', 300, 'Batch loss', 9.7703443e-05)\n",
      "('Step', 400, 'Batch loss', 0.00011068703)\n",
      "('Step', 500, 'Batch loss', 7.9527847e-05)\n",
      "('Step', 600, 'Batch loss', 9.6397256e-05)\n",
      "('Step', 700, 'Batch loss', 8.4277432e-05)\n",
      "('Step', 800, 'Batch loss', 7.9197118e-05)\n",
      "('Step', 900, 'Batch loss', 0.00011446494)\n",
      "('Step', 1000, 'Batch loss', 7.4930067e-05)\n",
      "('Step', 1100, 'Batch loss', 7.7164841e-05)\n",
      "('Step', 1200, 'Batch loss', 7.4381147e-05)\n",
      "('Step', 1300, 'Batch loss', 8.7803397e-05)\n",
      "('Step', 1400, 'Batch loss', 6.9332469e-05)\n",
      "('Step', 1500, 'Batch loss', 8.507711e-05)\n",
      "('Step', 1600, 'Batch loss', 0.000118889)\n",
      "('Step', 1700, 'Batch loss', 9.0503614e-05)\n",
      "('Step', 1800, 'Batch loss', 7.0988113e-05)\n",
      "('Training accuracy: ', 1.0)\n",
      "('Validation accuracy: ', 1.0)\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    tf.set_random_seed(22)\n",
    "    sess.run([tf.global_variables_initializer(), tf.local_variables_initializer()])\n",
    "    loss_list = []\n",
    "\n",
    "    # train our model\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        print(\"Epoch\", epoch_idx)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = start_idx + batch_size\n",
    "\n",
    "            x_data = x_train[start_idx:end_idx]\n",
    "            y_data = y_train[start_idx:end_idx]\n",
    "\n",
    "            _total_loss, _train_step, batchP = sess.run(\n",
    "                [total_loss, train_step, pred],\n",
    "                feed_dict={\n",
    "                    x_batch: x_data,\n",
    "                    y_batch: y_data\n",
    "                })\n",
    "\n",
    "            loss_list.append(_total_loss)\n",
    "\n",
    "            if batch_idx%100 == 0:\n",
    "                print(\"Step\",batch_idx, \"Batch loss\", _total_loss)\n",
    "\n",
    "        train_acc = sess.run(acc, { x_batch: x_train, y_batch: y_train })\n",
    "        validation_acc = sess.run(acc, { x_batch: x_val, y_batch: y_val })\n",
    "        print ('Training accuracy: ', train_acc)\n",
    "        print ('Validation accuracy: ', validation_acc)\n",
    "    \n",
    "    saver.save(sess, 'my-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(50, 13) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=(1, 13) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_lstm_cell/weights:0' shape=(63, 200) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_lstm_cell/biases:0' shape=(200,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable:0' shape=(50, 13) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=(1, 13) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_lstm_cell/weights:0' shape=(63, 200) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_lstm_cell/biases:0' shape=(200,) dtype=float32_ref>\n",
      "<tf.Variable 'Variable:0' shape=(50, 13) dtype=float32_ref>\n",
      "<tf.Variable 'Variable_1:0' shape=(1, 13) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_lstm_cell/weights:0' shape=(63, 200) dtype=float32_ref>\n",
      "<tf.Variable 'rnn/basic_lstm_cell/biases:0' shape=(200,) dtype=float32_ref>\n",
      "('s000000000097654321e  ', 's000000000097654321e  ')\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('my-model.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    x_batch = graph.get_tensor_by_name(\"x_batch:0\")\n",
    "    pred = graph.get_tensor_by_name(\"pred:0\")\n",
    "    \n",
    "    #for v in graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES):\n",
    "    #    print(v)\n",
    "        \n",
    "    w_lstm = graph.get_tensor_by_name('rnn/basic_lstm_cell/weights:0')\n",
    "    b_lstm = graph.get_tensor_by_name('rnn/basic_lstm_cell/biases:0')\n",
    "    w2 = graph.get_tensor_by_name('Variable:0')\n",
    "    b2 = graph.get_tensor_by_name('Variable_1:0')\n",
    "    \n",
    "    # play with the model\n",
    "    # input any number (up to 20 digits!) and get the predicted output\n",
    "    q = 123456789999999999\n",
    "    query = 's'+ str(q)[::-1] + 'e' + ' '*(MAXLEN-len(str(q))-2)\n",
    "    x = np.reshape(ctable.encode(query, MAXLEN),(1,MAXLEN,C))\n",
    "    p = sess.run(pred, { x_batch: x })\n",
    "    q += 1\n",
    "    gtruth = 's'+str(q)[::-1]+'e'+(' ')*(MAXLEN-len(str(q))-2)\n",
    "    pred = ''.join([ctable.indices_char[c] for c in p[0]])\n",
    "    print (gtruth, pred)\n",
    "    if gtruth!=pred:\n",
    "        print ('Wrong!')\n",
    "    else:\n",
    "        print ('Correct!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
