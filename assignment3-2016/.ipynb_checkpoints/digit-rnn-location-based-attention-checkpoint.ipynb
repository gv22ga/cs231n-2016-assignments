{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cs231n import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from cs231n.layers import *\n",
    "from cs231n.rnn_layers import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "('Total addition questions:', 10000)\n",
      "Vectorization...\n",
      "Training Data:\n",
      "(9000, 22, 13)\n",
      "(9000, 22)\n",
      "Validation Data:\n",
      "(1000, 22, 13)\n",
      "(1000, 22)\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 10000\n",
    "DIGITS = 20\n",
    "INVERT = False\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 2\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789se '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, 12))))\n",
    "    a = f()\n",
    "    b = a\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = (a, b)\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = 's{}e'.format(a)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = 's{}e'.format(b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans = ans + ' ' * (MAXLEN - len(ans))\n",
    "    if INVERT:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        ans= ans[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.int32)\n",
    "y = np.zeros((len(questions), MAXLEN), dtype=np.int32)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = np.array([ctable.char_indices[z] for z in sentence])\n",
    "\n",
    "    \n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "# because it is sorted\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32), array([12,  4,  3,  8,  4, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "print (x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = len(chars)\n",
    "C = len(chars)\n",
    "T = MAXLEN\n",
    "N = 5\n",
    "H = 50\n",
    "num_epochs = 5\n",
    "num_batches = x_train.shape[0]//N\n",
    "batch_size = N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 13)\n",
      "(?, 13)\n",
      "(?, 13)\n",
      "(?, 13)\n",
      "(?, 2, 22)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [None,T,D], name='x_batch')\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [None,T], name='y_batch')\n",
    "\n",
    "N = tf.shape(batchX_placeholder)[0]\n",
    "cell_state = tf.zeros([N,H], dtype=tf.float32)\n",
    "hidden_state = tf.zeros([N,H], dtype=tf.float32)\n",
    "init_state = tf.contrib.rnn.LSTMStateTuple(cell_state, hidden_state)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(H,C),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,C)), dtype=tf.float32)\n",
    "\n",
    "Wc = tf.Variable(np.random.rand(H,T),dtype=tf.float32, name='wc')\n",
    "bc = tf.Variable(np.zeros((1,T)), dtype=tf.float32, name='bc')\n",
    "\n",
    "# Forward passes\n",
    "with tf.variable_scope('rnn1'):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(H, state_is_tuple=True)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "    states_series, current_state = tf.nn.dynamic_rnn(cell, batchX_placeholder, initial_state=init_state)\n",
    "\n",
    "losses = []\n",
    "attn = []\n",
    "predictions = []\n",
    "with tf.variable_scope('rnn2'):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(H, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "    current_input = tf.zeros([N,D], dtype=tf.float32)\n",
    "    current_input += (ctable.encode('s',1)).astype(np.float32)\n",
    "    for t in range(T):\n",
    "        \n",
    "        c = tf.tanh(tf.matmul(current_state.h, Wc) + bc)\n",
    "        a = tf.nn.softmax(c)\n",
    "        b = tf.reshape(a, [N,T,1])\n",
    "        attn_i = tf.reduce_sum(tf.multiply(batchX_placeholder,b), axis=-2)\n",
    "        \n",
    "        if t > 0: tf.get_variable_scope().reuse_variables()\n",
    "        states_series, current_state = cell(tf.concat([attn_i, current_input], axis=1), current_state)\n",
    "        logits = tf.matmul(states_series, W2) + b2\n",
    "        labels = tf.reshape(batchY_placeholder[:,t], [-1])\n",
    "        losses.append(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        pred = tf.one_hot(tf.argmax(prob, axis=1), C, axis=-1)\n",
    "        predictions.append(pred)\n",
    "        current_input = pred\n",
    "        attn.append(a)\n",
    "\n",
    "predictions = tf.stack(predictions, axis=1, name='pred')\n",
    "attn = tf.stack(attn, axis=1, name='attn')\n",
    "print (attn.shape)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('New data, epoch', 0)\n",
      "('Step', 0, 'Batch loss', 2.5920718)\n",
      "('Step', 100, 'Batch loss', 1.0083702)\n",
      "('Step', 200, 'Batch loss', 0.98436469)\n",
      "('Step', 300, 'Batch loss', 0.9187482)\n",
      "('Step', 400, 'Batch loss', 0.92751986)\n",
      "('Step', 500, 'Batch loss', 0.60124868)\n",
      "('Step', 600, 'Batch loss', 0.57215738)\n",
      "('Step', 700, 'Batch loss', 0.80673951)\n",
      "('Step', 800, 'Batch loss', 0.79101765)\n",
      "('Step', 900, 'Batch loss', 0.82681119)\n",
      "('Step', 1000, 'Batch loss', 0.69836384)\n",
      "('Step', 1100, 'Batch loss', 0.76754332)\n",
      "('Step', 1200, 'Batch loss', 0.74343234)\n",
      "('Step', 1300, 'Batch loss', 0.46418551)\n",
      "('Step', 1400, 'Batch loss', 0.74924511)\n",
      "('Step', 1500, 'Batch loss', 0.70714533)\n",
      "('Step', 1600, 'Batch loss', 0.82639432)\n",
      "('Step', 1700, 'Batch loss', 0.66895509)\n",
      "('Validation accuracy', 0.003)\n",
      "('Training accuracy', 0.004)\n",
      "('New data, epoch', 1)\n",
      "('Step', 0, 'Batch loss', 0.68389964)\n",
      "('Step', 100, 'Batch loss', 0.67136472)\n",
      "('Step', 200, 'Batch loss', 0.64643979)\n",
      "('Step', 300, 'Batch loss', 0.55507427)\n",
      "('Step', 400, 'Batch loss', 0.64558965)\n",
      "('Step', 500, 'Batch loss', 0.30855304)\n",
      "('Step', 600, 'Batch loss', 0.26778758)\n",
      "('Step', 700, 'Batch loss', 0.49890521)\n",
      "('Step', 800, 'Batch loss', 0.41429183)\n",
      "('Step', 900, 'Batch loss', 0.37902373)\n",
      "('Step', 1000, 'Batch loss', 0.28258109)\n",
      "('Step', 1100, 'Batch loss', 0.29547057)\n",
      "('Step', 1200, 'Batch loss', 0.17936511)\n",
      "('Step', 1300, 'Batch loss', 0.091315739)\n",
      "('Step', 1400, 'Batch loss', 0.19307067)\n",
      "('Step', 1500, 'Batch loss', 0.05506422)\n",
      "('Step', 1600, 'Batch loss', 0.16442138)\n",
      "('Step', 1700, 'Batch loss', 0.097668149)\n",
      "('Validation accuracy', 0.53)\n",
      "('Training accuracy', 0.5192222222222223)\n",
      "('New data, epoch', 2)\n",
      "('Step', 0, 'Batch loss', 0.086709216)\n",
      "('Step', 100, 'Batch loss', 0.13211219)\n",
      "('Step', 200, 'Batch loss', 0.12232649)\n",
      "('Step', 300, 'Batch loss', 0.086954094)\n",
      "('Step', 400, 'Batch loss', 0.073339984)\n",
      "('Step', 500, 'Batch loss', 0.026490942)\n",
      "('Step', 600, 'Batch loss', 0.064290717)\n",
      "('Step', 700, 'Batch loss', 0.11631954)\n",
      "('Step', 800, 'Batch loss', 0.069616944)\n",
      "('Step', 900, 'Batch loss', 0.069466896)\n",
      "('Step', 1000, 'Batch loss', 0.012359564)\n",
      "('Step', 1100, 'Batch loss', 0.030213829)\n",
      "('Step', 1200, 'Batch loss', 0.029745273)\n",
      "('Step', 1300, 'Batch loss', 0.0077296309)\n",
      "('Step', 1400, 'Batch loss', 0.05186262)\n",
      "('Step', 1500, 'Batch loss', 0.031089012)\n",
      "('Step', 1600, 'Batch loss', 0.018759394)\n",
      "('Step', 1700, 'Batch loss', 0.031048393)\n",
      "('Validation accuracy', 0.824)\n",
      "('Training accuracy', 0.8497777777777777)\n",
      "('New data, epoch', 3)\n",
      "('Step', 0, 'Batch loss', 0.042539857)\n",
      "('Step', 100, 'Batch loss', 0.031715728)\n",
      "('Step', 200, 'Batch loss', 0.037829939)\n",
      "('Step', 300, 'Batch loss', 0.017976318)\n",
      "('Step', 400, 'Batch loss', 0.067637853)\n",
      "('Step', 500, 'Batch loss', 0.013587161)\n",
      "('Step', 600, 'Batch loss', 0.015144864)\n",
      "('Step', 700, 'Batch loss', 0.023042878)\n",
      "('Step', 800, 'Batch loss', 0.026934359)\n",
      "('Step', 900, 'Batch loss', 0.014529638)\n",
      "('Step', 1000, 'Batch loss', 0.0040034112)\n",
      "('Step', 1100, 'Batch loss', 0.038929269)\n",
      "('Step', 1200, 'Batch loss', 0.011586761)\n",
      "('Step', 1300, 'Batch loss', 0.011039814)\n",
      "('Step', 1400, 'Batch loss', 0.0082291085)\n",
      "('Step', 1500, 'Batch loss', 0.014803748)\n",
      "('Step', 1600, 'Batch loss', 0.0088684019)\n",
      "('Step', 1700, 'Batch loss', 0.033244248)\n",
      "('Validation accuracy', 0.906)\n",
      "('Training accuracy', 0.9134444444444444)\n",
      "('New data, epoch', 4)\n",
      "('Step', 0, 'Batch loss', 0.021083457)\n",
      "('Step', 100, 'Batch loss', 0.016350191)\n",
      "('Step', 200, 'Batch loss', 0.031966023)\n",
      "('Step', 300, 'Batch loss', 0.0056569232)\n",
      "('Step', 400, 'Batch loss', 0.009014722)\n",
      "('Step', 500, 'Batch loss', 0.0042702286)\n",
      "('Step', 600, 'Batch loss', 0.0088255489)\n",
      "('Step', 700, 'Batch loss', 0.032936264)\n",
      "('Step', 800, 'Batch loss', 0.01298182)\n",
      "('Step', 900, 'Batch loss', 0.0052770651)\n",
      "('Step', 1000, 'Batch loss', 0.0049067265)\n",
      "('Step', 1100, 'Batch loss', 0.004601392)\n",
      "('Step', 1200, 'Batch loss', 0.010964817)\n",
      "('Step', 1300, 'Batch loss', 0.011663406)\n",
      "('Step', 1400, 'Batch loss', 0.0044739507)\n",
      "('Step', 1500, 'Batch loss', 0.0047229603)\n",
      "('Step', 1600, 'Batch loss', 0.014023862)\n",
      "('Step', 1700, 'Batch loss', 0.0051571899)\n",
      "('Validation accuracy', 0.923)\n",
      "('Training accuracy', 0.9287777777777778)\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        loss_list = []\n",
    "\n",
    "        for epoch_idx in range(num_epochs):\n",
    "            print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "\n",
    "                batchX = x_train[start_idx:end_idx]\n",
    "                batchY = y_train[start_idx:end_idx]\n",
    "\n",
    "                _total_loss, _train_step, batchP = sess.run(\n",
    "                    [total_loss, train_step, predictions],\n",
    "                    feed_dict={\n",
    "                        batchX_placeholder: batchX,\n",
    "                        batchY_placeholder: batchY\n",
    "                    })\n",
    "\n",
    "                loss_list.append(_total_loss)\n",
    "\n",
    "                if batch_idx%100 == 0:\n",
    "                    print(\"Step\",batch_idx, \"Batch loss\", _total_loss)\n",
    "                    #plot(loss_list, _predictions_series, batchX, batchY)\n",
    "            \n",
    "            batchX = x_val\n",
    "            batchY = y_val\n",
    "\n",
    "            batchP = sess.run(\n",
    "                predictions,\n",
    "                feed_dict={\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY\n",
    "                })\n",
    "            w = [''.join(ctable.indices_char[i] for i in batchY[j]) for j in range(x_val.shape[0])]\n",
    "            q = [ctable.decode(batchP[i]) for i in range(x_val.shape[0])]\n",
    "            correct_examples = 0\n",
    "            for i,j in zip(w,q):\n",
    "                if (i==j):\n",
    "                    correct_examples += 1\n",
    "            print ('Validation accuracy', float(correct_examples)/x_val.shape[0])\n",
    "            \n",
    "            batchX = x_train\n",
    "            batchY = y_train\n",
    "\n",
    "            batchP = sess.run(\n",
    "                predictions,\n",
    "                feed_dict={\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY\n",
    "                })\n",
    "            w = [''.join(ctable.indices_char[i] for i in batchY[j]) for j in range(x_train.shape[0])]\n",
    "            q = [ctable.decode(batchP[i]) for i in range(x_train.shape[0])]\n",
    "            correct_examples = 0\n",
    "            for i,j in zip(w,q):\n",
    "                if (i==j):\n",
    "                    correct_examples += 1\n",
    "            print ('Training accuracy', float(correct_examples)/x_train.shape[0])\n",
    "            \n",
    "        saver.save(sess, 'my-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my-model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAD8CAYAAAAIXLzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XtwnFeZ5/HfI1myLMsX+SbLkuNLYjuxgxODcUIxVQkh\nDOYSAuwUS1hSngI2VBZqQw1bS4ACpmaYIlC1zDC1FIMJKbLAOrAhA9mBzZAEciFJhchJSJw4xrYi\n25Ity5bvN8myzv6hbqXPc15JLVtqyfb3U5VSP93n9HtaSqKj933e57EQggAAwMWtbKwXAAAAxh4b\nAgAAwIYAAACwIQAAAGJDAAAAxIYAAACIDQEAABAbAgAAoHPcEJjZWjPbYmbbzOzOkVoUAAAoLTvb\nSoVmVi7pz5LeJalV0nOSbgkhvDrQnJqamjBz5sz+OOvY/rmJEydG8enTp5M5vb29fm3JmDNnzgz6\nPv49JKm8vHzQuKKiIpnT3d0dxWVl8Z4ra23+2MW874QJE5IxXuF6Ozs7dfTo0fTgAABIGvq3ysDW\nSNoWQmiWJDO7T9LNkgbcEMycOVNf/vKX+2P/S06Surq6onjx4sVR3N7ensw5fvx4FFdVVSVjDh06\nFMVtbW1RfOrUqWRObW1tFE+ZMiWK6+vrkzk7d+6M4urq6ij2mwop/cxz5sxJxuzatWvQMVkbjZqa\nmv7HX//615PXAQDIO5dLBg2SCn9LteaeAwAA55lRTyo0s9vMrMnMmo4dOzbahwMAAGfhXC4ZtEma\nXxA35p6LhBDWS1ovSY2NjeHAgQP9rx08eDB5046Ojih+7rnnorjwNHjeddddF8Vz585NxixcuDCK\nt23bFsUPPPBAMmfq1KlRfPnll0exP9UvScuWLRv0OFnq6uqiuLW1NRmzZMmSQd8ja07hJZmenp4h\n1wEAuHidyxmC5yQtMbNFZlYp6aOSHhyZZQEAgFI66zMEIYQeM/uspH+XVC7pnhDCKyO2MgAAUDLn\ncslAIYTfSPrNCK0FAACMESoVAgCAcztDMFw9PT3av39/f1xZWZmMeetb3xrFnZ2dUezv65fSGgM7\nduxIxjzxxBNR7AsVzZ8/X960adOi2NcC2Lp1azLHJyI2NjZGcVby34kTJ6I46zMWft+ktJ5BVqGi\nwpoNWQWdAADI4wwBAABgQwAAANgQAAAAlTiHwMyivIEjR44kYzZt2hTFvseAzxeQpI9//ONRvHLl\nymTMk08+GcXNzc1RnFUk6S1veUsU+/yG2bNnJ3N8/oJvzpSVN+FzFbK+L359fozv+SDFxZj8OgAA\nKMQZAgAAwIYAAACwIQAAABqDHILC++UrKiqSMS+88EIU+w6J73jHO5I5N910UxQ/9dRTyZjJkydH\n8fXXXx/FWdf2fd2BlpaWKA4hJHM8n/OQVS9g+/btUexrGUhpwyb/fcmqM1DYKIo6BACAwXCGAAAA\nsCEAAABsCAAAgNgQAAAAlTipUJLKyt7YgyxdujR53Sf/XXbZZVGcVbTn9ttvj+LDhw8nY1atWhXF\nvrlR1hyfuFeM2traKD5+/PiQ7+kbIGUlAB44cCCKq6qqojiradL06dP7H5vZACsGAIAzBAAAQGwI\nAACA2BAAAACVOIegoqJC9fX1/XFTU1My5uabb47iurq6KP7BD36QzPF5BytWrEjGzJs3L4p9waCT\nJ08O+b6+wVBh86C8rVu3RrEvKFRTU5PM2bNnTxT7hk5S+pl8boJvqiRJkyZN6n9MDgEAYDCcIQAA\nAGwIAAAAGwIAACA2BAAAQCVOKuzq6oqS7hYsWJCM8clyviDPmjVrkjm+SI8vOiRJ+/bti2Kf/OeP\nK0nV1dVRvHz58ih+7bXXkjm+U2F5eXkyxvMdEC+99NJkjC/I5N938eLFyZyenp7+x4UFoQAA8Pgt\nAQAA2BAAAAA2BAAAQCXOISgvL9eMGTP646wCPD5nYOPGjVE8e/bsZM5QeQeSNG3atCj2BY/2798/\nwKrfsH379iHHdHd3R/HRo0ejeMqUKckcnwNReO0/z+cMdHZ2RvHEiROTOYVFoMghAAAMht8SAACA\nDQEAAGBDAAAANAY5BIXX0C+55JJkjL+WX9igR5JefvnlZI6/jz+rgZCvD5B1zd3zTYd8/oLPS5Ck\nvXv3RvGsWbOi2NdDkNKcgaw6Cj73wL+vr5kgSS0tLf2PfW4DAACFOEMAAADYEAAAgCI2BGZ2j5l1\nmNmmgudmmNnDZrY197V2dJcJAABGUzFnCH4kaa177k5Jj4YQlkh6NBcDAIDz1JBJhSGEJ8xsoXv6\nZknX5x7fK+kxSV8Y6r16enqipMHm5uZkzO7du6O4oaEhitetW5fM8YWJHnvssWSMb2Y0efLkKG5s\nbEzm+IZBfo5/TylN7vNrq6ioSOYcOnQoiufOnZuM8d+rysrKKM5KcCw8dm9vb/I6AAB5Z5tDUBdC\nyKfgt0uqG2wwAAAY3845qTCEECSFgV43s9vMrMnMmk6cOHGuhwMAAKPgbDcEe82sXpJyXzsGGhhC\nWB9CWB1CWJ11rzwAABh7Z1uY6EFJ6yTdlfv6q2ImVVZWRsWIspoQLVu2LIr9tW/f1CdrzNKlS5Mx\nHR3xnsVf2z948GAyxxfz8fkMvimRJM2fPz+KfXEjn4cgSQsXLozirDyDJUuWRLEvgOTXJsWFlLLW\nCgBAXjG3HW6Q9IykZWbWamafVN9G4F1mtlXSjbkYAACcp4q5y+CWAV565wivBQAAjBEqFQIAgNI2\nNzp9+nR07bupqSkZ46+V33TTTVE8ffr0ZM7TTz8dxY888kgyxtcUuOKKK6K4q6srmVNWNvh+KauJ\nUmFDISm9dl9Mk6Gs+ga+AZJP0NyxY0cy59ixY/2Psz4fAAB5nCEAAABsCAAAABsCAAAgNgQAAEAl\nTiosKyuLmvKsWrUqGfPMM89E8YsvvhjFWYV9fDGgO+64Ixlz6aWXRrEvirR58+ZkzrZt2wY9zqlT\np5I5c+bMiWJfmGjq1KnJHF9s6W1ve1syZsuWLVFcWOBJks6cOZPMMbP+x5MmTUpeBwAgjzMEAACA\nDQEAAGBDAAAAVOIcgsrKyug6fFYBnk996lNR3NbWNuT7TpgQf4xnn302GfONb3wjinfu3BnFK1eu\nHHJOeXl5FD/55JPJHF8gyOcdvP7668kcXwApKzehvr4+in2hoqzvU2G+QlaOAQAAeZwhAAAAbAgA\nAAAbAgAAIMlCCCU72Lx580JhjsCCBQuSMf4a/BNPPBHFvqmPlDYuWrRoUTJm1qxZUXzixIko9rUA\npPRafmtraxTPmDFjyOMcPHgwin39AElRw6es40rS0qVLo7ijoyOK/eeRpHnz5vU//ta3vqWdO3da\nMggAAHGGAAAAiA0BAAAQGwIAACA2BAAAQCUuTCSlxX28mTNnRvGVV14Zxb4IkST5xEjflEhKE/cK\nE+6k7KZDviHQ8ePHo7impiaZ09vbG8W1tbVRnJUw2NDQMOhxJOnPf/5zFM+dOzeKZ8+encx55ZVX\n+h93dXUlrwMAkMcZAgAAwIYAAACwIQAAACpxDsHEiROjokH79u1Lxpw8eTKKp0yZEsWXXXZZMmfP\nnj1R3NzcnIyZPn16FL/22mtR7AsKSdLatWuj+L3vfW8Ub9y4MZkzceLEKPafMavJkFlcLygrT8Ln\nGfjmRv49JOm6667rf/zrX/86eR0AgDzOEAAAADYEAACADQEAAFCJcwi6u7vV0tLSH7/5zW9Oxvhr\n/Zs3b47iZ555JpnjmyRdddVVyZgDBw5E8bvf/e4orqioSOb4Yx05ciQZ482fPz+Kd+3aFcWnT59O\n5ixZsmTQtUppU6fu7u4h37ew5kHW6wAA5HGGAAAAsCEAAABsCAAAgNgQAAAAlTip0MyihkGvvvpq\nMubGG2+M4qEaDEnSc889F8XTpk1LxvjmP74w0Zw5c5I5vlGRLyrk1yZJmzZtGnSML7QkpY2Xst7X\nJwX6pML6+vpkzv79+/sf+88CAEAhzhAAAAA2BAAAoIgNgZnNN7Pfm9mrZvaKmd2Re36GmT1sZltz\nX2tHf7kAAGA0FJND0CPp8yGE581siqSNZvawpL+W9GgI4S4zu1PSnZK+MNgblZeXq6ampj9+/fXX\nkzGPPvpoFHd1dUXxBz7wgWSOLyCUdb3cFzyaMWNGFB87diyZ09bWFsX+On15eXkyZ+nSpVFcWIhJ\nko4ePZrM8fkNWev3TZ98TsGWLVuSOYWNoLLWCgBA3pBnCEIIe0IIz+ceH5W0WVKDpJsl3Zsbdq+k\nD47WIgEAwOgaVg6BmS2UtErSs5LqQgj5vsPtkupGdGUAAKBkit4QmFmNpF9I+lwIISrqH0IIksIA\n824zsyYza8o6LQ8AAMZeURsCM6tQ32bgpyGEB3JP7zWz+tzr9ZI6suaGENaHEFaHEFYX5g8AAIDx\nY8ikQjMzST+UtDmE8O2Clx6UtE7SXbmvvxrqvcrKyjR58uT++Morr0zGbNiwIYqXLVsWxe95z3uS\nOT5Rr7OzMxnjiyD5MVkFgz784Q9H8YQJ8bdr27ZtyZyJEydGcWVlZRRndVX075tVfMkXK/KfuTCB\nMK+9vb3/MYWJAACDKeYug7dLulXSy2b2Yu65L6lvI/BzM/ukpB2SPjI6SwQAAKNtyA1BCOEPkmyA\nl985sssBAABjgUqFAACgtM2Nent7owI7VVVVyZiPfexjyZxCDz/8cDJnx44dUeybEEl9jZUGG7N4\n8eJkzksvvRTFPmegoaEhmXPixIko9uuvrU0LOvoiSX03bcR8boJ/nz/96U/JnMLPVFbG3g8AMDB+\nSwAAADYEAACADQEAAFCJcwi6urq0ffv2/ri7uzsZ4+/T97G/r19Kr6f7+/ql9Nq+bw60adOmZM6s\nWbOi2DdI8nHWWnbv3h3FWdfyOzrimk5ZFR0bGxujuKenJ4qz8hkK6xlQh+D8YWYzJP1M0kJJLZI+\nEkI4mDHujKSXc+HOEELa+QsAisQZAmD8uVN9nUSXSHo0F2c5GUK4OvcPmwEA54QNATD+0EkUQMmV\n9JIBgKIU20m0ysyaJPVIuiuE8MusQWZ2m6TbJKmsrOwtvgz2YApLjQ/l4MHkqsagZs+eXfTYrJLf\ngxnOZ/SXD4dSXV1d9NjC26xH2rRp00btvSVp//79RY8d7s8+qzz7SMm67Xwgw/n3ROq77F0sf6v7\nUIbz73jWremD6erq2h9CGPI/ODYEwBgws0ckzc146cuFQQghmNlA//UvCCG0mdliSb8zs5dDCNv9\noBDCeknrJammpiZcffXVRa9zzZo1RY+97777ih4rSbfffnvRY+vqhtdd/aqrrip6bGtr67Dee/Xq\n1UWPzcpNGsxwcn3e//73D+u9h/tL5O677y567P333z+s9/7jH/9Y9Njh/mIdzubE98oZSlb/moFk\n5bINZu7crP8dZBvuJnb79u07hh5V4g1BdXW1Vq5c2R8/++yzyZhPfOITUfzb3/42ih966KFkTltb\nWxRn/Q/vM5/5TBT7JL2WlpZkzuHDh6PYJ/9l/Ys3b968KF60aFEUHzhwIJnjd52XXHJJMsYf2//l\nllXkqXBMeXl58jrGTgjhxoFeM7N8J9GrJP1PSZPN7M4Qwl1u6H4z+5mkt0iaLOkvJX1vtNYM4MJG\nDgEw/jwo6a8lfVfSA5L+WdItZrY8P8DMaiV9WtJBSdeq77IBiYUAzhqXDIDx5y5JD6nvksKb1ddJ\ntFPSfzGzqhDCpyRdIekf1Jdj8HZJX5X0DTOzMNxzwwAgzhAA404IoVPSNyVtCCHcGEI4IKk199qn\ncl+fVl/b8XeEEN4UQviBpMOSZo7RsgGc50p6huD48ePauHHjGwfPSLrYsGFDFNfX10fx2rVrkzlH\njx5NjuM98MADUewznLPW4rM+/XV8XxxISost+fyGrDn+OFlJNHv37h10TlaGauFaRjPbGeNb4V0G\nvkkWAORxhgAYn9okzS+IG3PPZY4xswmSpqnv0kIkhLA+hLA6hLB6uJnPAC4e/N8BGJ+ek7TSzJol\n9arvLgJ/Z8Je9d1uuEXSdEmvkz8A4GxxhgAYn/K/2C33jyQFM/s7M8vfTfCk+s4S1EjqkPQfS7tE\nABeSkp4hqKqqigpB7Nu3LxnjC5D4ZkBDNfGRpOeffz4Z46tu+SIQWdfgfZEQH2ddl/f1APx6fZMl\nSVq6dGkUb968ORlTU1MTxb7xUtZa5s9/44zzL3+ZWcQO49caSS+FEN4tSWb2RUk3hxC+WjDmtKSH\nQgifHYsFAriwcIYAGJ8aJO0qiFtzz3n/wcxeMrP7zWx+xusAUBRyCIDz1/9V362JXWb2afU1QrrB\nDyq8y0DSsaeeempLxnvNkpQUr3/qqadGcLmxr3zlK6P23oPI/JwXmIvhM0oj8DlffPHFEVrKuTt0\n6NBAL43Ez3NBMYPYEADj05B3GeTqFeTdLelbWW9U2MtgIGbWFEIovkj/eepi+JwXw2eU+JyjgUsG\nwPj0nKQlZrbIzColfVR9JY375fod5H1AUpp8AgBFKukZgt7e3ij5LauAUFNTUxT7RkXTp09P5viC\nQVkdrHwS4ZQpU6K4MAEvzxcDam9vj2Kf6Je1vqlTp0ZxVvJfc3NzFPuGSFLanMgXY8oqOFOYtJlV\nEAnjVwihx8w+K+nfJZVLuieE8IqZ/Z2kphDCg5L+a+6Ogx5JB9TX/wAAzgqXDIBxKoTwG0m/cc99\nteDxFyV9cYQON+glhQvIxfA5L4bPKPE5RxyXDADk8wwueBfD57wYPqPE5xwNbAgAAEBpLxmUl5dH\n19gXLEjvhPDXyrdsie+Q+slPfpLM8TkDXV1dyRjfJKmysjKKW1pakjkvv/xyFM+YMSOKJ0+enMzx\n7+vzDhYuXJjMOXz4cBT74kZSmiNQTEOnwvVR0RZZzGytpO+oL0/h7hDCXWO8pFFhZi2Sjko6I6nn\nQslON7N7JL1fUkcI4crcczMk/UzSQkktkj4SQjg4VmscCQN8zr+V9J8l5ZOlvpS7zHZeytUR+V+S\n6tRXqXR9COE7pfx5coYAuEiZWbmk70p6j6Tlkm4xs+Vju6pR9Y4QwtUXymYg50eSfAvYOyU9GkJY\nIunRXHy++5HSzylJ/5j7mV59Pm8GcnokfT6EsFzStZI+k/vvsWQ/TzYEwMVrjaRtIYTmEEK3pPsk\n3TzGa8IwhBCeUN8dJoVuVl+RKuW+frCkixoFA3zOC0oIYU8I4fnc46Pqu424QSX8ebIhAC5exZZH\nvhAESb81s425yo0XsroQwp7c43b1nYK+UH02V7r7HjOrHevFjBQzWyhplaRnVcKfZ0lzCLq7u7Vj\nx47+2NcPyOLv41+xYkUy5vTp01Gcdc/9a6+9FsU+V8HXJZCGrinQ3d2dzPHX8n0DJ9+USJImTZo0\n6BxJOnAg3hz7/AX/eaQ4F8E3iQIuMn8RQmgzszmSHjaz13J/dV7QQgjBzC7UBKLvSfp79W32/l7S\n/5D0iTFd0QgwsxpJv5D0uRDCETPrf220f578lgAuXkOWR75QhBDacl87JP2r+i6XXKj25qtY5r4O\n/ZfXeSiEsDeEcCaE0CvpB7oAfqZmVqG+zcBPQwgP5J4u2c+TDQFw8RqyPPKFwMwmm9mU/GNJfylp\n09iualQ9KGld7vE6Sb8aw7WMGle6+0M6z3+m1ncq4IeSNocQvl3wUsl+nkNeMjCzKklPSJqYG39/\nCOFrZrZIfUlIMyVtlHRrLjEJwHlgoPLIY7ys0VAn6V9zp14nSPrfIYSHxnZJI8PMNki6XtIsM2uV\n9DVJd0n6uZl9UtIOSR8ZuxWOjAE+5/VmdrX6Lhm0SPr0mC1wZLxd0q2SXjazfBvGL6mEP89icgi6\nJN0QQjiWO53xBzP7f5L+Rn23fNxnZv8i6ZPqu6YD4DyRVR75QhNCaJZ01VivYzSEEG4Z4KV3lnQh\no2yAz/nDki9kFIUQ/iDJBni5JD/PITcEoa+izbFcWJH7J6iv7/rHcs/fK+lvNcSGoKKiImoy5JP0\nJKm1tTWKfeLe5ZdfnszxCXaPP/54MsYnGs6ZMyeKswr3+EJEx44di2KfzChJdXV1g445derUkGvz\nTZUkaeXKlVG8eXPc2G7mzJnJnMLiRWfOnEleBwAgr6gcAjMrz53C6JD0sKTtkg6FEPK/yS7k25UA\nALjgFbUhyGVyXq2+LOQ1ktI/0wdgZreZWZOZNfm/sAEAwPgwrLsMQgiHJP1e0tskTTez/CWHAW9X\nCiGsDyGsDiGsrqmpOafFAgCA0VHMXQazJZ0OIRwys0mS3iXpm+rbGPyV+u40KOpWiJMnT+qVV95I\nYvZNiSSptjYuNuWL9LzwwgvJHF/sp7OzMxnT1hbvV/zZiiVLliRzKioqotjnM2QVA9q/f38U+4JA\nWQ2RfM6AL4gkSYcOHYpi/5mz8hkmTHjjx1tY3AIAAK+YuwzqJd2ba4RSJunnIYR/M7NXJd1nZl+X\n9IIusIxPAAAuJsXcZfCS+moq++ebdQFUhgIAAFQqBAAAYkMAAABU4m6H1dXVuvrqq/tjn+gnpclv\nDQ1xeYOsAjs+iXDBggXJGF/YZ9euXVHsEwglqaurK4p9R8TCpL08nxDY3t4exYXFgvIWL14cxVmF\niQq7RGbN8R0TpfgzZnWABAAgjzMEAACADQEAAGBDAAAAVOIcgu7u7ui69sGDB5MxvnBPYc6BJG3a\nlLa8PnDgQBT7a/1ZzzU2Nkbx8ePHkzmHDx+O4okTJ0bxiRMnkjk+B8IXM/J5CZJ05MiRKC5sAJVX\nVVUVxb29vVG8bdu2ZE5h8SJfIAkAgEL8lgAAAGwIAAAAGwIAAKAS5xCEEKJr6pdfnnZR9vfgb9iw\nIYr9tXMprSHgr/VLaZMkf/3fNw+SJN+d0R8nK1fBNzfydQey1lZXVzfkWvw8n4swY8aMZE5hbQJy\nCAAAg+G3BAAAYEMAAADYEAAAALEhAAAAKnFSYXl5eZSId+rUqWSMf66wuI4kHTt2LJnjC/kUk0A3\nc+bMKM5KEPSNlLKO7VVXV0exL7Q0b968ZI5PRPQFkbLep76+PoqziiTV1tb2P85qxAQAQB5nCAAA\nABsCAADAhgAAAKjEOQRnzpyJCvW0tLQkY/y18qEaDElSW1tbFC9fvjwZs2LFiihubm6O4oaGhmSO\nb1S0Z8+eKK6srEzm+Gv5/ji+2JGUNlbKygfw3xffEKkwXyCvs7Oz/3FPT0/yOgAAeZwhAAAAbAgA\nAAAbAgAAoBLnEFRUVET34V9zzTXJGH/N3d9vv2nTpmSOb3h0+vTpZMyuXbui2Ocm+PyA/HoL+QZC\n/rq+JFVVVUWxr13gmx1JQ9dekLKbOhXyzY6kOK+gvLx80PkAgIsbZwgAAAAbAgAAwIYAAACIDQEA\nAFCJkwpPnz6t3bt3R7F36NChKL700kujuLW1NZmzb9++KPbJgFJaeOiyyy6L4qykvccffzyKfTGg\nrGZHU6dOjWLfuGjSpEnJHP8Z29vbkzG+0dLJkycHXZs/dlbSIQAAeZwhAAAAbAgAAAAbAgAAoBLn\nEJSVlam6uro/XrlyZTLmkUceieKdO3dG8Q033JDM8cV+fve73yVjXnjhhSieNm1aFBcWTMrzjZQa\nGxujuJgmRHV1dVHscwwkqaOjI4p9syNJWrRoURR3d3dHcVbeRGHxJd+oCQCAQpwhAAAAbAgAAMAw\nNgRmVm5mL5jZv+XiRWb2rJltM7OfmVnl6C0TAACMpuHkENwhabOk/EXwb0r6xxDCfWb2L5I+Kel7\ng72BzyHYvHlzMsY3GaqpqYnirIZC06dPj+IPfehDgy1DkvT8889HcdZ9+r7p0IEDB6LY1wLI0tPT\nE8VXXHFFMsbXGJgyZUoyxtds8HUTtm/fnszxzZgAABhIUWcIzKxR0vsk3Z2LTdINku7PDblX0gdH\nY4EAAGD0FXvJ4J8k/XdJ+T9LZ0o6FELI//nbKqkha6KZ3WZmTWbWlJU9DwAAxt6QGwIze7+kjhDC\nxrM5QAhhfQhhdQhhddbpfgAAMPaKySF4u6QPmNl7JVWpL4fgO5Kmm9mE3FmCRklto7dMAAAwmobc\nEIQQvijpi5JkZtdL+m8hhP9kZv9H0l9Juk/SOkm/KuaAhQVyspobXXLJJVHsmwP5JENJ2rFjRxS/\n733vS8b4IkNPP/10FN91113JHJ+cWFVVFcXl5eXJHJ/IN3v27Cj2iYpSWlRo7969yRh/7MLkTCm7\n8FDhsbKaNwEAkHcudQi+IOlvzGyb+nIKfjgySwIAAKU2rNLFIYTHJD2We9wsac3ILwkAAJQalQoB\nAIAshFCyg82dOzfceuut/fGb3vSmZIxvGOSbEPmCQlLaACmrIM+KFSui2N8CmXUN/siRI1Hsr8P7\n6/iSVFkZF2z077FgwYJkjv8ZNDc3J2N8boX/PvniTJK0ZcuW/sc//vGP1d7eTocjAEAmzhAAAAA2\nBAAAgA0BAADQMO8yGGn+2r8kHTt2LIoXL14cxbNmzUrm+Ov//jq+lDYv8vUBfC0ASeru7o5if60/\nqyZCZ2dnFPtaC/7zSWlzozlz5iRj/PfK5wxk1TcorL2QlSMBAEAeZwgAAAAbAgAAwIYAAACIDQEA\nAFCJkwonTJgQJfPV1tYmYw4fPhzF9fX1UTxv3rxkjm+A5JsdZY3xyX1ZyX7t7e1RvHDhwij2xYEk\nybd4nj9//qDvKaWfafv27ckYnxToExp3796dzClcH82NAACD4QwBAABgQwAAANgQAAAAlTiHwMyi\nokFZjYquuOKKKN68eXMUZ10L942Kshr9VFVVRbEvOpRVDMgXK/IFhPx7SOm1/tbW1ihubGxM5rS1\ntUXxkiVLkjGvvvpqFPtCRFOmTEnmFBYmKi8vT14HACCPMwQAAIANAQAAYEMAAADEhgAAAKjESYVl\nZWWaNGlSf5zVYdB3Jbz22mujOKtDoi/2U3iMvEOHDkXx1q1bozirMNHcuXOj2BcZ8smMWaZNmxbF\nWZ0Yi+lc6NfiCyBlFUk6cuRI/2PfqREAgEKcIQAAAGwIAAAAGwIAAKAS5xB0dXVF1+6zrpX7a+FN\nTU1RXFYW/ihOAAADgklEQVSW7mH8dfnOzs5kTOH1dElqaGiI4urq6mSOzyvw+Q1ZOQT+Wv/UqVOj\nOOtav19/VnMm3wDJFzPyjZektKETAAAD4QwBAABgQwAAANgQAAAAjUEdAn//vOfvl/cNhbJyCNau\nXRvFS5cuTcbcfffdUfz9738/iletWpXMef3116N48eLFUZzVaGn37t1R7JsbLVq0KJnT0dERxVn5\nDHv27Ilin4uQVUfh8OHD/Y/99xEAgEKcIQAAAGwIAAAAGwIAACA2BAAAQCVOKuzt7Y2S4XyxHSlt\n2uOL9lRVVSVz7r///iieMWNGMub06dNRvG7duihuaWlJ5lxzzTVR3NPTE8XLli1L5vjkP1+Y6ODB\ng8mcrKJCnk809E2eamtrkzmzZ8/ufzxhQkl/1ACA8wxnCAAAABsCAADAhgAAAEgyXwhoVA9mtk/S\nDkmzJJ0vnXfOp7VKA693QQhhdsbzAACUdkPQf1CzphDC6pIf+CycT2uVzr/1AgDGBy4ZAAAANgQA\nAGDsNgTrx+i4Z+N8Wqt0/q0XADAOjEkOAQAAGF+4ZAAAAEq7ITCztWa2xcy2mdmdpTx2MczsHjPr\nMLNNBc/NMLOHzWxr7mtaI3gMmNl8M/u9mb1qZq+Y2R2558flegEA41vJNgRmVi7pu5LeI2m5pFvM\nbHmpjl+kH0la6567U9KjIYQlkh7NxeNBj6TPhxCWS7pW0mdy38/xul4AwDhWyjMEayRtCyE0hxC6\nJd0n6eYSHn9IIYQnJB1wT98s6d7c43slfbCkixpACGFPCOH53OOjkjZLatA4XS8AYHwr5YagQdKu\ngrg199x4VxdCyLcwbJdUN5aLyWJmCyWtkvSszoP1AgDGH5IKhyH03ZIxrm7LMLMaSb+Q9LkQwpHC\n18bjegEA41MpNwRtkuYXxI2558a7vWZWL0m5rx1jvJ5+Zlahvs3AT0MID+SeHrfrBQCMX6XcEDwn\naYmZLTKzSkkflfRgCY9/th6UtC73eJ2kX43hWvqZmUn6oaTNIYRvF7w0LtcLABjfSt3t8L2S/klS\nuaR7Qgj/ULKDF8HMNki6Xn0dA/dK+pqkX0r6uaRL1Nep8SMhBJ94WHJm9heSnpT0sqTe3NNfUl8e\nwbhbLwBgfKNSIQAAIKkQAACwIQAAAGJDAAAAxIYAAACIDQEAABAbAgAAIDYEAABAbAgAAICk/w+Y\nQYqvjfdJAgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac8654fb50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac80affb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!!!!!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('my-model.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    a = 123456\n",
    "    q = 's'+ str(a) + 'e' + ' '*(MAXLEN-len(str(a))-2)\n",
    "    batchX = np.reshape(ctable.encode(q, MAXLEN),(1,MAXLEN,C))\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    x_batch = graph.get_tensor_by_name(\"x_batch:0\")\n",
    "    pred = graph.get_tensor_by_name(\"pred:0\")\n",
    "    attn = graph.get_tensor_by_name(\"attn:0\")\n",
    "    wc = graph.get_tensor_by_name(\"wc:0\")\n",
    "    bc = graph.get_tensor_by_name(\"bc:0\")\n",
    "    \n",
    "    Wc, Bc = sess.run(\n",
    "                [wc, bc]\n",
    "            )\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(Wc)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(Bc)\n",
    "    plt.gcf().set_size_inches(10, 4)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    for i in range(1):\n",
    "        if i%1== 0:\n",
    "            print (i)\n",
    "        batchP, batchA = sess.run(\n",
    "                [pred,attn] ,\n",
    "                feed_dict={\n",
    "                    x_batch: batchX\n",
    "                })\n",
    "        #a += 1\n",
    "        b = 's'+str(a)+'e'+(' ')*(MAXLEN-len(str(a))-2)\n",
    "        q = ctable.decode(batchP[0])\n",
    "        if b!=q:\n",
    "            print ('Failure at ',b,q)\n",
    "            q = b[::-1]\n",
    "        else:\n",
    "            q = q[::-1]\n",
    "        batchX = np.reshape(ctable.encode(' '*(MAXLEN-len(str(a))-2) + 's'+ str(a) + 'e', MAXLEN),(1,MAXLEN,C))\n",
    "        \n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.imshow(batchA[0])\n",
    "        plt.gcf().set_size_inches(10, 4)\n",
    "        plt.show()\n",
    "    \n",
    "    print ('SUCCESS!!!!!')\n",
    "    \n",
    "    '''\n",
    "    w = [''.join(ctable.indices_char[i] for i in batchY[j]) for j in range(x_train.shape[0])]\n",
    "    q = [ctable.decode(batchP[i]) for i in range(x_train.shape[0])]\n",
    "    correct_examples = 0\n",
    "    for i,j in zip(w,q):\n",
    "        if (i==j):\n",
    "            correct_examples += 1\n",
    "        else :\n",
    "            print (i,j)\n",
    "    \n",
    "    print ('Validation accuracy', float(correct_examples)/x_train.shape[0])\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
