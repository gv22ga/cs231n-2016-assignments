{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from cs231n import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from cs231n.layers import *\n",
    "from cs231n.rnn_layers import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "('Total addition questions:', 10000)\n",
      "Vectorization...\n",
      "Training Data:\n",
      "(9000, 9, 13)\n",
      "(9000, 9)\n",
      "Validation Data:\n",
      "(1000, 9, 13)\n",
      "(1000, 9)\n"
     ]
    }
   ],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "\n",
    "\n",
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 10000\n",
    "DIGITS = 7\n",
    "INVERT = True\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 2\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789se '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    # generate data one less than specified digit length so that we can have input and output of same size\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, DIGITS))))\n",
    "    a = f()\n",
    "    b = a+1\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = (a, b)\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = 's{}e'.format(a)\n",
    "    query = ' ' * (MAXLEN - len(q)) + q\n",
    "    ans = 's{}e'.format(b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans = ' ' * (MAXLEN - len(ans)) + ans\n",
    "    if INVERT:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        ans = ans[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.int32)\n",
    "y = np.zeros((len(questions), MAXLEN ), dtype=np.int32)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = np.array([ctable.char_indices[z] for z in sentence])\n",
    "\n",
    "    \n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "# because it is sorted\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = len(chars)\n",
    "C = len(chars)\n",
    "T = MAXLEN\n",
    "N = 5\n",
    "H = 150\n",
    "num_epochs = 5\n",
    "num_batches = x_train.shape[0]//N\n",
    "batch_size = N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 9, 13)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [None,T,D], name='x_batch')\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [None,T], name='y_batch')\n",
    "\n",
    "N = tf.shape(batchX_placeholder)[0]\n",
    "cell_state = tf.zeros([N,H], dtype=tf.float32)\n",
    "hidden_state = tf.zeros([N,H], dtype=tf.float32)\n",
    "init_state = tf.contrib.rnn.LSTMStateTuple(cell_state, hidden_state)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(H,C),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,C)), dtype=tf.float32)\n",
    "\n",
    "# Forward passes\n",
    "with tf.variable_scope('rnn1'):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(H, state_is_tuple=True)\n",
    "    states_series, current_state = tf.nn.dynamic_rnn(cell, batchX_placeholder, initial_state=init_state)\n",
    "\n",
    "losses = []\n",
    "predictions = []\n",
    "with tf.variable_scope('rnn2'):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(H, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "    current_input = tf.zeros([N,D], dtype=tf.float32)\n",
    "    current_input += (ctable.encode('s',1)).astype(np.float32)\n",
    "    for t in range(T):\n",
    "        if t > 0: tf.get_variable_scope().reuse_variables()\n",
    "        states_series, current_state = cell(tf.stack(current_input), current_state)\n",
    "        logits = tf.matmul(states_series, W2) + b2 #Broadcasted addition\n",
    "        labels = tf.reshape(batchY_placeholder[:,t], [-1])\n",
    "        losses.append(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        pred = tf.one_hot(tf.argmax(prob, axis=1), C, axis=-1)\n",
    "        current_input = pred\n",
    "        predictions.append(pred)\n",
    "\n",
    "predictions = tf.stack(predictions, axis=1, name='pred')\n",
    "print (predictions.shape)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('New data, epoch', 0)\n",
      "('Step', 0, 'Batch loss', 2.5285707)\n",
      "('Step', 100, 'Batch loss', 1.7185457)\n",
      "('Step', 200, 'Batch loss', 1.4127274)\n",
      "('Step', 300, 'Batch loss', 1.2988453)\n",
      "('Step', 400, 'Batch loss', 1.0313483)\n",
      "('Step', 500, 'Batch loss', 0.97316241)\n",
      "('Step', 600, 'Batch loss', 0.94167984)\n",
      "('Step', 700, 'Batch loss', 0.65194541)\n",
      "('Step', 800, 'Batch loss', 0.69190645)\n",
      "('Step', 900, 'Batch loss', 0.61823708)\n",
      "('Step', 1000, 'Batch loss', 1.820441)\n",
      "('Step', 1100, 'Batch loss', 0.71741045)\n",
      "('Step', 1200, 'Batch loss', 0.53464592)\n",
      "('Step', 1300, 'Batch loss', 0.66126275)\n",
      "('Step', 1400, 'Batch loss', 0.57788455)\n",
      "('Step', 1500, 'Batch loss', 0.39050192)\n",
      "('Step', 1600, 'Batch loss', 0.41159782)\n",
      "('Step', 1700, 'Batch loss', 0.49800003)\n",
      "('Validation accuracy', 0.274)\n",
      "('Training accuracy', 0.2897777777777778)\n",
      "('New data, epoch', 1)\n",
      "('Step', 0, 'Batch loss', 0.46082979)\n",
      "('Step', 100, 'Batch loss', 0.36494273)\n",
      "('Step', 200, 'Batch loss', 0.3436076)\n",
      "('Step', 300, 'Batch loss', 0.24188267)\n",
      "('Step', 400, 'Batch loss', 0.22371814)\n",
      "('Step', 500, 'Batch loss', 0.19201119)\n",
      "('Step', 600, 'Batch loss', 0.14888027)\n",
      "('Step', 700, 'Batch loss', 0.15490454)\n",
      "('Step', 800, 'Batch loss', 0.18047996)\n",
      "('Step', 900, 'Batch loss', 0.12740435)\n",
      "('Step', 1000, 'Batch loss', 0.16988978)\n",
      "('Step', 1100, 'Batch loss', 0.24501692)\n",
      "('Step', 1200, 'Batch loss', 0.031040523)\n",
      "('Step', 1300, 'Batch loss', 0.16925053)\n",
      "('Step', 1400, 'Batch loss', 0.084530249)\n",
      "('Step', 1500, 'Batch loss', 0.036785737)\n",
      "('Step', 1600, 'Batch loss', 0.10730578)\n",
      "('Step', 1700, 'Batch loss', 0.070950113)\n",
      "('Validation accuracy', 0.802)\n",
      "('Training accuracy', 0.8347777777777777)\n",
      "('New data, epoch', 2)\n",
      "('Step', 0, 'Batch loss', 0.050362289)\n",
      "('Step', 100, 'Batch loss', 0.2012388)\n",
      "('Step', 200, 'Batch loss', 0.035332814)\n",
      "('Step', 300, 'Batch loss', 0.03154793)\n",
      "('Step', 400, 'Batch loss', 0.024607539)\n",
      "('Step', 500, 'Batch loss', 0.018622428)\n",
      "('Step', 600, 'Batch loss', 0.23650129)\n",
      "('Step', 700, 'Batch loss', 0.029165143)\n",
      "('Step', 800, 'Batch loss', 0.0079574995)\n",
      "('Step', 900, 'Batch loss', 0.13078372)\n",
      "('Step', 1000, 'Batch loss', 0.096245393)\n",
      "('Step', 1100, 'Batch loss', 0.014421218)\n",
      "('Step', 1200, 'Batch loss', 0.0040691225)\n",
      "('Step', 1300, 'Batch loss', 0.026923468)\n",
      "('Step', 1400, 'Batch loss', 0.068899974)\n",
      "('Step', 1500, 'Batch loss', 0.0085645197)\n",
      "('Step', 1600, 'Batch loss', 0.034207281)\n",
      "('Step', 1700, 'Batch loss', 0.028153136)\n",
      "('Validation accuracy', 0.943)\n",
      "('Training accuracy', 0.9638888888888889)\n",
      "('New data, epoch', 3)\n",
      "('Step', 0, 'Batch loss', 0.016526498)\n",
      "('Step', 100, 'Batch loss', 0.080426984)\n",
      "('Step', 200, 'Batch loss', 0.019497085)\n",
      "('Step', 300, 'Batch loss', 0.0024365457)\n",
      "('Step', 400, 'Batch loss', 0.0065154494)\n",
      "('Step', 500, 'Batch loss', 0.0013994341)\n",
      "('Step', 600, 'Batch loss', 0.018310554)\n",
      "('Step', 700, 'Batch loss', 0.013460248)\n",
      "('Step', 800, 'Batch loss', 0.0015256392)\n",
      "('Step', 900, 'Batch loss', 0.14431055)\n",
      "('Step', 1000, 'Batch loss', 0.092305683)\n",
      "('Step', 1100, 'Batch loss', 0.0032380628)\n",
      "('Step', 1200, 'Batch loss', 0.0030978031)\n",
      "('Step', 1300, 'Batch loss', 0.010190146)\n",
      "('Step', 1400, 'Batch loss', 0.0048050159)\n",
      "('Step', 1500, 'Batch loss', 0.0034537299)\n",
      "('Step', 1600, 'Batch loss', 0.0050151399)\n",
      "('Step', 1700, 'Batch loss', 0.0029488252)\n",
      "('Validation accuracy', 0.971)\n",
      "('Training accuracy', 0.9876666666666667)\n",
      "('New data, epoch', 4)\n",
      "('Step', 0, 'Batch loss', 0.0076957834)\n",
      "('Step', 100, 'Batch loss', 0.0074943257)\n",
      "('Step', 200, 'Batch loss', 0.015339472)\n",
      "('Step', 300, 'Batch loss', 0.0010772054)\n",
      "('Step', 400, 'Batch loss', 0.005706111)\n",
      "('Step', 500, 'Batch loss', 0.00058531901)\n",
      "('Step', 600, 'Batch loss', 0.0036034624)\n",
      "('Step', 700, 'Batch loss', 0.0028532406)\n",
      "('Step', 800, 'Batch loss', 0.0012245292)\n",
      "('Step', 900, 'Batch loss', 0.010355645)\n",
      "('Step', 1000, 'Batch loss', 0.010434393)\n",
      "('Step', 1100, 'Batch loss', 0.0010908176)\n",
      "('Step', 1200, 'Batch loss', 0.0041631963)\n",
      "('Step', 1300, 'Batch loss', 0.0042975368)\n",
      "('Step', 1400, 'Batch loss', 0.014096013)\n",
      "('Step', 1500, 'Batch loss', 0.0045369053)\n",
      "('Step', 1600, 'Batch loss', 0.0015037996)\n",
      "('Step', 1700, 'Batch loss', 0.0035761092)\n",
      "('Validation accuracy', 0.982)\n",
      "('Training accuracy', 0.9964444444444445)\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        loss_list = []\n",
    "\n",
    "        for epoch_idx in range(num_epochs):\n",
    "            print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "\n",
    "                batchX = x_train[start_idx:end_idx]\n",
    "                batchY = y_train[start_idx:end_idx]\n",
    "\n",
    "                _total_loss, _train_step, batchP = sess.run(\n",
    "                    [total_loss, train_step, predictions],\n",
    "                    feed_dict={\n",
    "                        batchX_placeholder: batchX,\n",
    "                        batchY_placeholder: batchY\n",
    "                    })\n",
    "\n",
    "                loss_list.append(_total_loss)\n",
    "\n",
    "                if batch_idx%100 == 0:\n",
    "                    print(\"Step\",batch_idx, \"Batch loss\", _total_loss)\n",
    "                    #plot(loss_list, _predictions_series, batchX, batchY)\n",
    "\n",
    "            batchX = x_val\n",
    "            batchY = y_val\n",
    "\n",
    "            batchP = sess.run(\n",
    "                predictions,\n",
    "                feed_dict={\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY\n",
    "                })\n",
    "            w = [''.join(ctable.indices_char[i] for i in batchY[j]) for j in range(x_val.shape[0])]\n",
    "            q = [ctable.decode(batchP[i]) for i in range(x_val.shape[0])]\n",
    "            correct_examples = 0\n",
    "            for i,j in zip(w,q):\n",
    "                if (i==j):\n",
    "                    correct_examples += 1\n",
    "            print ('Validation accuracy', float(correct_examples)/x_val.shape[0])\n",
    "\n",
    "            batchX = x_train\n",
    "            batchY = y_train\n",
    "\n",
    "            batchP = sess.run(\n",
    "                predictions,\n",
    "                feed_dict={\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY\n",
    "                })\n",
    "            w = [''.join(ctable.indices_char[i] for i in batchY[j]) for j in range(x_train.shape[0])]\n",
    "            q = [ctable.decode(batchP[i]) for i in range(x_train.shape[0])]\n",
    "            correct_examples = 0\n",
    "            for i,j in zip(w,q):\n",
    "                if (i==j):\n",
    "                    correct_examples += 1\n",
    "            print ('Training accuracy', float(correct_examples)/x_train.shape[0])\n",
    "\n",
    "        saver.save(sess, 'my-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my-model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "('Failure at ', 'e002s    ', 'e001s    ')\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "('Failure at ', 'e009s    ', 'e008s    ')\n",
      "900\n",
      "('Failure at ', 'e0001s   ', 'e000s    ')\n",
      "SUCCESS!!!!!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('my-model.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    q = '      s1e'\n",
    "    a = 1\n",
    "    batchX = np.reshape(ctable.encode(q, MAXLEN),(1,MAXLEN,C))\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    x_batch = graph.get_tensor_by_name(\"x_batch:0\")\n",
    "    pred = graph.get_tensor_by_name(\"pred:0\")\n",
    "    \n",
    "    for i in range(1000):\n",
    "        if i%100== 0:\n",
    "            print (i)\n",
    "        batchP = sess.run(\n",
    "                pred,\n",
    "                feed_dict={\n",
    "                    x_batch: batchX\n",
    "                })\n",
    "        a += 1\n",
    "        b = 'e'+str(a)[::-1]+'s'+(' ')*(9-len(str(a))-2)\n",
    "        q = ctable.decode(batchP[0])\n",
    "        if b!=q:\n",
    "            print ('Failure at ',b,q)\n",
    "            q = b[::-1]\n",
    "        else:\n",
    "            q = q[::-1]\n",
    "        batchX = np.reshape(ctable.encode(q, MAXLEN),(1,MAXLEN,C))\n",
    "    \n",
    "    print ('SUCCESS!!!!!')\n",
    "    \n",
    "    '''\n",
    "    w = [''.join(ctable.indices_char[i] for i in batchY[j]) for j in range(x_train.shape[0])]\n",
    "    q = [ctable.decode(batchP[i]) for i in range(x_train.shape[0])]\n",
    "    correct_examples = 0\n",
    "    for i,j in zip(w,q):\n",
    "        if (i==j):\n",
    "            correct_examples += 1\n",
    "        else :\n",
    "            print (i,j)\n",
    "    \n",
    "    print ('Validation accuracy', float(correct_examples)/x_train.shape[0])\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (tf.reverse([[1,2,3]],-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
