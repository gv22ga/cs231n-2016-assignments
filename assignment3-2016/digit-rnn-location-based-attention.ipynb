{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cs231n import optim\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "from cs231n.layers import *\n",
    "from cs231n.rnn_layers import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"Initialize character table.\n",
    "        # Arguments\n",
    "            chars: Characters that can appear in the input.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "\n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"One hot encode given string C.\n",
    "        # Arguments\n",
    "            num_rows: Number of rows in the returned one hot encoding. This is\n",
    "                used to keep the # of rows for each data the same.\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "\n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "('Total addition questions:', 10000)\n",
      "Vectorization...\n",
      "Training Data:\n",
      "(9000, 22, 13)\n",
      "(9000, 22)\n",
      "Validation Data:\n",
      "(1000, 22, 13)\n",
      "(1000, 22)\n"
     ]
    }
   ],
   "source": [
    "# Parameters for the model and dataset.\n",
    "TRAINING_SIZE = 10000\n",
    "DIGITS = 20\n",
    "INVERT = False\n",
    "\n",
    "# Maximum length of input is 'int + int' (e.g., '345+678'). Maximum length of\n",
    "# int is DIGITS.\n",
    "MAXLEN = DIGITS + 2\n",
    "\n",
    "# All the numbers, plus sign and space for padding.\n",
    "chars = '0123456789se '\n",
    "ctable = CharacterTable(chars)\n",
    "\n",
    "questions = []\n",
    "expected = []\n",
    "seen = set()\n",
    "print('Generating data...')\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                    for i in range(np.random.randint(1, 12))))\n",
    "    a = f()\n",
    "    b = a\n",
    "    # Skip any addition questions we've already seen\n",
    "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
    "    key = (a, b)\n",
    "    if key in seen:\n",
    "        continue\n",
    "    seen.add(key)\n",
    "    # Pad the data with spaces such that it is always MAXLEN.\n",
    "    q = 's{}e'.format(a)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = 's{}e'.format(b)\n",
    "    # Answers can be of maximum size DIGITS + 1.\n",
    "    ans = ans + ' ' * (MAXLEN - len(ans))\n",
    "    if INVERT:\n",
    "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
    "        # space used for padding.)\n",
    "        ans= ans[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "print('Total addition questions:', len(questions))\n",
    "\n",
    "print('Vectorization...')\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.int32)\n",
    "y = np.zeros((len(questions), MAXLEN), dtype=np.int32)\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = np.array([ctable.char_indices[z] for z in sentence])\n",
    "\n",
    "    \n",
    "# Shuffle (x, y) in unison as the later parts of x will almost all be larger\n",
    "# digits.\n",
    "# because it is sorted\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Explicitly set apart 10% for validation data that we never train over.\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32), array([12,  4,  3,  8,  4, 10, 11,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  0,  0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "print (x_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = len(chars)\n",
    "C = len(chars)\n",
    "T = MAXLEN\n",
    "N = 5\n",
    "H = 50\n",
    "num_epochs = 5\n",
    "num_batches = x_train.shape[0]//N\n",
    "batch_size = N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 22, 22)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "batchX_placeholder = tf.placeholder(tf.float32, [None,T,D], name='x_batch')\n",
    "batchY_placeholder = tf.placeholder(tf.int32, [None,T], name='y_batch')\n",
    "\n",
    "N = tf.shape(batchX_placeholder)[0]\n",
    "cell_state = tf.zeros([N,H], dtype=tf.float32)\n",
    "hidden_state = tf.zeros([N,H], dtype=tf.float32)\n",
    "init_state = tf.contrib.rnn.LSTMStateTuple(cell_state, hidden_state)\n",
    "\n",
    "W2 = tf.Variable(np.random.rand(H,C),dtype=tf.float32)\n",
    "b2 = tf.Variable(np.zeros((1,C)), dtype=tf.float32)\n",
    "\n",
    "Wc = tf.Variable(np.random.rand(H,T),dtype=tf.float32, name='wc')\n",
    "bc = tf.Variable(np.zeros((1,T)), dtype=tf.float32, name='bc')\n",
    "\n",
    "# Forward passes\n",
    "with tf.variable_scope('rnn1'):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(H, state_is_tuple=True)\n",
    "    #cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "    states_series, current_state = tf.nn.dynamic_rnn(cell, batchX_placeholder, initial_state=init_state)\n",
    "\n",
    "losses = []\n",
    "attn = []\n",
    "predictions = []\n",
    "with tf.variable_scope('rnn2'):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(H, state_is_tuple=True, reuse=tf.get_variable_scope().reuse)\n",
    "    #cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "    current_input = tf.zeros([N,D], dtype=tf.float32)\n",
    "    current_input += (ctable.encode('s',1)).astype(np.float32)\n",
    "    for t in range(T):\n",
    "        \n",
    "        c = tf.tanh(tf.matmul(current_state.h, Wc) + bc)\n",
    "        a = tf.nn.softmax(c)\n",
    "        b = tf.reshape(a, [N,T,1])\n",
    "        attn_i = tf.reduce_sum(tf.multiply(batchX_placeholder,b), axis=-2)\n",
    "        \n",
    "        if t > 0: tf.get_variable_scope().reuse_variables()\n",
    "        states_series, current_state = cell(tf.concat([attn_i, current_input], axis=1), current_state)\n",
    "        logits = tf.matmul(states_series, W2) + b2\n",
    "        labels = tf.reshape(batchY_placeholder[:,t], [-1])\n",
    "        losses.append(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n",
    "        prob = tf.nn.softmax(logits)\n",
    "        pred = tf.one_hot(tf.argmax(prob, axis=1), C, axis=-1)\n",
    "        predictions.append(pred)\n",
    "        current_input = pred\n",
    "        attn.append(a)\n",
    "\n",
    "predictions = tf.stack(predictions, axis=1, name='pred')\n",
    "attn = tf.stack(attn, axis=1, name='attn')\n",
    "print (attn.shape)\n",
    "total_loss = tf.reduce_mean(losses)\n",
    "train_step = tf.train.AdagradOptimizer(0.3).minimize(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('New data, epoch', 0)\n",
      "('Step', 0, 'Batch loss', 2.5542836)\n",
      "('Step', 100, 'Batch loss', 1.5750238)\n",
      "('Step', 200, 'Batch loss', 1.2699211)\n",
      "('Step', 300, 'Batch loss', 0.74067146)\n",
      "('Step', 400, 'Batch loss', 0.8095988)\n",
      "('Step', 500, 'Batch loss', 0.60331637)\n",
      "('Step', 600, 'Batch loss', 0.6529308)\n",
      "('Step', 700, 'Batch loss', 0.8695035)\n",
      "('Step', 800, 'Batch loss', 0.53027385)\n",
      "('Step', 900, 'Batch loss', 0.43653715)\n",
      "('Step', 1000, 'Batch loss', 0.6428864)\n",
      "('Step', 1100, 'Batch loss', 0.49383268)\n",
      "('Step', 1200, 'Batch loss', 0.72695577)\n",
      "('Step', 1300, 'Batch loss', 0.59112251)\n",
      "('Step', 1400, 'Batch loss', 0.38127917)\n",
      "('Step', 1500, 'Batch loss', 0.43739784)\n",
      "('Step', 1600, 'Batch loss', 0.37329984)\n",
      "('Step', 1700, 'Batch loss', 0.67683703)\n",
      "('Validation accuracy', 0.527)\n",
      "('Training accuracy', 0.5477777777777778)\n",
      "('New data, epoch', 1)\n",
      "('Step', 0, 'Batch loss', 0.21668474)\n",
      "('Step', 100, 'Batch loss', 0.21020064)\n",
      "('Step', 200, 'Batch loss', 0.36926723)\n",
      "('Step', 300, 'Batch loss', 0.11497609)\n",
      "('Step', 400, 'Batch loss', 0.046094242)\n",
      "('Step', 500, 'Batch loss', 0.042518251)\n",
      "('Step', 600, 'Batch loss', 0.048881356)\n",
      "('Step', 700, 'Batch loss', 0.029449767)\n",
      "('Step', 800, 'Batch loss', 0.017570522)\n",
      "('Step', 900, 'Batch loss', 0.010183376)\n",
      "('Step', 1000, 'Batch loss', 0.058415152)\n",
      "('Step', 1100, 'Batch loss', 0.016932324)\n",
      "('Step', 1200, 'Batch loss', 0.020935776)\n",
      "('Step', 1300, 'Batch loss', 0.012588898)\n",
      "('Step', 1400, 'Batch loss', 0.0052823662)\n",
      "('Step', 1500, 'Batch loss', 0.014373481)\n",
      "('Step', 1600, 'Batch loss', 0.011067938)\n",
      "('Step', 1700, 'Batch loss', 0.011660529)\n",
      "('Validation accuracy', 0.989)\n",
      "('Training accuracy', 0.9896666666666667)\n",
      "('New data, epoch', 2)\n",
      "('Step', 0, 'Batch loss', 0.0093090385)\n",
      "('Step', 100, 'Batch loss', 0.10889642)\n",
      "('Step', 200, 'Batch loss', 0.082257397)\n",
      "('Step', 300, 'Batch loss', 0.006551255)\n",
      "('Step', 400, 'Batch loss', 0.006993942)\n",
      "('Step', 500, 'Batch loss', 0.0035813614)\n",
      "('Step', 600, 'Batch loss', 0.0045381682)\n",
      "('Step', 700, 'Batch loss', 0.0047332896)\n",
      "('Step', 800, 'Batch loss', 0.0031663706)\n",
      "('Step', 900, 'Batch loss', 0.0021592977)\n",
      "('Step', 1000, 'Batch loss', 0.0040480262)\n",
      "('Step', 1100, 'Batch loss', 0.0023912124)\n",
      "('Step', 1200, 'Batch loss', 0.0043307869)\n",
      "('Step', 1300, 'Batch loss', 0.0030845283)\n",
      "('Step', 1400, 'Batch loss', 0.0017237373)\n",
      "('Step', 1500, 'Batch loss', 0.0036590889)\n",
      "('Step', 1600, 'Batch loss', 0.0032744592)\n",
      "('Step', 1700, 'Batch loss', 0.0035463294)\n",
      "('Validation accuracy', 0.995)\n",
      "('Training accuracy', 0.9985555555555555)\n",
      "('New data, epoch', 3)\n",
      "('Step', 0, 'Batch loss', 0.0031549977)\n",
      "('Step', 100, 'Batch loss', 0.0036612533)\n",
      "('Step', 200, 'Batch loss', 0.0054695439)\n",
      "('Step', 300, 'Batch loss', 0.0021476196)\n",
      "('Step', 400, 'Batch loss', 0.0024335666)\n",
      "('Step', 500, 'Batch loss', 0.0013354711)\n",
      "('Step', 600, 'Batch loss', 0.001959909)\n",
      "('Step', 700, 'Batch loss', 0.0020203374)\n",
      "('Step', 800, 'Batch loss', 0.001419308)\n",
      "('Step', 900, 'Batch loss', 0.000820279)\n",
      "('Step', 1000, 'Batch loss', 0.0017811185)\n",
      "('Step', 1100, 'Batch loss', 0.0011033388)\n",
      "('Step', 1200, 'Batch loss', 0.0019825487)\n",
      "('Step', 1300, 'Batch loss', 0.0014350436)\n",
      "('Step', 1400, 'Batch loss', 0.00083051366)\n",
      "('Step', 1500, 'Batch loss', 0.0018264139)\n",
      "('Step', 1600, 'Batch loss', 0.0014860712)\n",
      "('Step', 1700, 'Batch loss', 0.0017310044)\n",
      "('Validation accuracy', 0.997)\n",
      "('Training accuracy', 0.9993333333333333)\n",
      "('New data, epoch', 4)\n",
      "('Step', 0, 'Batch loss', 0.0020169122)\n",
      "('Step', 100, 'Batch loss', 0.0018808789)\n",
      "('Step', 200, 'Batch loss', 0.0033068515)\n",
      "('Step', 300, 'Batch loss', 0.0013229173)\n",
      "('Step', 400, 'Batch loss', 0.0014988483)\n",
      "('Step', 500, 'Batch loss', 0.00080904539)\n",
      "('Step', 600, 'Batch loss', 0.001298792)\n",
      "('Step', 700, 'Batch loss', 0.0012314251)\n",
      "('Step', 800, 'Batch loss', 0.00087277673)\n",
      "('Step', 900, 'Batch loss', 0.00051782525)\n",
      "('Step', 1000, 'Batch loss', 0.0010607224)\n",
      "('Step', 1100, 'Batch loss', 0.00068179343)\n",
      "('Step', 1200, 'Batch loss', 0.0012760111)\n",
      "('Step', 1300, 'Batch loss', 0.000956417)\n",
      "('Step', 1400, 'Batch loss', 0.00054873544)\n",
      "('Step', 1500, 'Batch loss', 0.0012159839)\n",
      "('Step', 1600, 'Batch loss', 0.0010238802)\n",
      "('Step', 1700, 'Batch loss', 0.0011397928)\n",
      "('Validation accuracy', 0.998)\n",
      "('Training accuracy', 0.9995555555555555)\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    with tf.device(\"/cpu:0\"):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        loss_list = []\n",
    "\n",
    "        for epoch_idx in range(num_epochs):\n",
    "            print(\"New data, epoch\", epoch_idx)\n",
    "\n",
    "            for batch_idx in range(num_batches):\n",
    "                start_idx = batch_idx * batch_size\n",
    "                end_idx = start_idx + batch_size\n",
    "\n",
    "                batchX = x_train[start_idx:end_idx]\n",
    "                batchY = y_train[start_idx:end_idx]\n",
    "\n",
    "                _total_loss, _train_step, batchP = sess.run(\n",
    "                    [total_loss, train_step, predictions],\n",
    "                    feed_dict={\n",
    "                        batchX_placeholder: batchX,\n",
    "                        batchY_placeholder: batchY\n",
    "                    })\n",
    "\n",
    "                loss_list.append(_total_loss)\n",
    "\n",
    "                if batch_idx%100 == 0:\n",
    "                    print(\"Step\",batch_idx, \"Batch loss\", _total_loss)\n",
    "                    #plot(loss_list, _predictions_series, batchX, batchY)\n",
    "            \n",
    "            batchX = x_val\n",
    "            batchY = y_val\n",
    "\n",
    "            batchP = sess.run(\n",
    "                predictions,\n",
    "                feed_dict={\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY\n",
    "                })\n",
    "            w = [''.join(ctable.indices_char[i] for i in batchY[j]) for j in range(x_val.shape[0])]\n",
    "            q = [ctable.decode(batchP[i]) for i in range(x_val.shape[0])]\n",
    "            correct_examples = 0\n",
    "            for i,j in zip(w,q):\n",
    "                if (i==j):\n",
    "                    correct_examples += 1\n",
    "            print ('Validation accuracy', float(correct_examples)/x_val.shape[0])\n",
    "            \n",
    "            batchX = x_train\n",
    "            batchY = y_train\n",
    "\n",
    "            batchP = sess.run(\n",
    "                predictions,\n",
    "                feed_dict={\n",
    "                    batchX_placeholder: batchX,\n",
    "                    batchY_placeholder: batchY\n",
    "                })\n",
    "            w = [''.join(ctable.indices_char[i] for i in batchY[j]) for j in range(x_train.shape[0])]\n",
    "            q = [ctable.decode(batchP[i]) for i in range(x_train.shape[0])]\n",
    "            correct_examples = 0\n",
    "            for i,j in zip(w,q):\n",
    "                if (i==j):\n",
    "                    correct_examples += 1\n",
    "            print ('Training accuracy', float(correct_examples)/x_train.shape[0])\n",
    "            \n",
    "        saver.save(sess, 'my-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my-model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my-model\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgQAAAD8CAYAAAAIXLzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XmMldeZ5/HfQ7EUxVJsZbNDAQUGDGYLS7zIMYlN2pad\nTZl4RpFbkx5bmraSqDMZ24mUdLonGmekpDujRG6RxIrHSuJkbKeNE69xsDH2GFPsUOzFDkWxFTu1\nnvnjLrrnnLc2oG4tfD+Sxf3de86976US1/H7Pu9zzDknAABwY+vV2QcAAAA6HwsCAADAggAAALAg\nAAAAYkEAAADEggAAAIgFAQAAEAsCAACga1wQmNkyM9tpZnvM7MnrdVAAACC/7Go7FZpZgaRdkj4j\n6bCktZIeds5VNDensLDQDRgwIJuTPrugoMDLdXV1Xm5qaorm9O7d28tFRUXRmPr6+hbnJAmPL/zs\n/v37R3Nqa2u93KuXv+a6fPlyNKdPnz5ebmhoiMaE7xN+9qVLl6I5/fr1yz6uqanRxYsXLRoEAICk\n1n8rNm+hpD3OuUpJMrMXJD0kqdkFwYABA/TZz342m8Nf0pI0ZMgQLx84cMDLV65cieYMGzbMy3Pn\nzo3GHD9+3MsjRozwctLiJDy+ixcvevnWW2+N5uzbt8/L4eJk69at0ZybbrrJyydPnozGDB48uMXP\nLi8vj+ZMmTIl+/iZZ56JXgcAIONaLhmMkXQoJx9OPwcAALqZazlD0CZm9qikR6XkU/kAAKDzXcuC\n4IikcTl5bPo5j3NuuaTlkjRmzBg3derU7GtJp/9DjY2NXk5aVBw7dszLW7ZsicbkXk+XpOrqai9P\nmDAhmhMeX3htP+k0fXhqP/zcpNqFsK4gnCNJo0aN8nL4HZMuX6xZs6bZzwAAINe1XDJYK6nMzErN\nrK+kr0hacX0OCwAA5NNVnyFwzjWY2eOS3pRUIOlZ59y263ZkAAAgb66phsA595qk167TsQAAgE5C\np0IAANDxdxnkqq2t1f79+7P5woUL0ZjCwkIvL1iwwMuDBg2K5uT2NpCkzZs3R2Nqamq8fPjwYS+H\n/Q4kaeHChV4OC/OSChzD77RhwwYvT5o0KZpTUeG3bhg9enQ0JmzYFBY8JjUmKi4ubnY+AAC5OEMA\nAABYEAAAABYEAABAea4hMDPvWvbAgQOjMXfeeaeXV61a5eXhw4dHc8J6gKQmPGvXrvXyyJEjvTxn\nzpxoTm69gxQ3L0ralyD3ur0klZWVeXn37t3RnNw9B6TkvQzC2opp06Z5+ezZs9Gc3E2TzNjXCADQ\nPM4QAAAAFgQAAIAFAQAAUJ5rCPr3769Zs2Zlc3hdX5Kcc16eP3++l3M37Mmoq6vzclgfIEkPPPCA\nlzdu3OjlTZs2RXMGDBjg5fDa/tChQ6M5N910U7uPLdw0Kaw7kFrvVXDu3LloTu7f9Wuv0VASANA8\nzhAAAAAWBAAAgAUBAAAQCwIAAKA8FxXW1dXp4MGD2ZxUPHfmzBkv79u3z8thcyBJampq8nJuQ56M\nxsZGL5eWlnp59erVrc4JhQWQSZ8dbiqUtDlTbW2tl3P/jjLGjRvn5fPnz3s5LEyUpKqqquzj+vr6\n6HUAADI4QwAAAFgQAAAAFgQAAEB5riHo16+ft5FPUrOc6dOne/nee+/1ctL19VdffdXLYUMhKa5X\nCPO8efOiOYcOHfJySUmJl6urq6M5/fv393LYzCjp2I4dO+blYcOGRWPC58LNisIGSOGcsJYBAIBc\nnCEAAAAsCAAAAAsCAAAgFgQAAEB5Liqsr6/XkSNHsvnxxx+PxoS7ED7xxBNeLiwsjObMnj3by2Fz\nI0mqqanxcm7THkk6depUNCfcuTBsBjRq1KhoTljcd+HCBS8nHX+/fv28PHny5GhMeXm5lydNmuTl\nsHGR5BcShkWIAADk4gwBAABgQQAAAFgQAAAA5bmGoKCgQMXFxdn8xhtvRGPC5j8LFy708uDBg6M5\n4Zhw4yJJ+u1vf+vlV155xctFRUXRnPC6+/jx470c1gdIcd1B+H3C2gUprnkYO3ZsNCbcAGn9+vVe\nXrBgQTRny5Yt2ccXL16MXgcAIIMzBAAAgAUBAABgQQAAAJTnGgLJvzc+aaOf8D798Lr9tm3bojlh\nD4Hca+cZYV3BnDlzvLxr165ozu7du73cq5e/fpowYUI0p3dv/6807FUQ1gJI0tKlS728c+fOaMwd\nd9zh5fr6ei/v27cvmpNbV/Dee+9FrwMAkMEZAgAAwIIAAAC0YUFgZs+aWbWZbc15bpiZvW1mu9N/\nDu3YwwQAAB2pLWcIfi1pWfDck5Lecc6VSXonnQEAQDfValGhc26VmU0Mnn5I0t3px89JelfSE2pF\nuLnRxInh28YFgrlFiJL08ssvR3NefPFFL+d+RkZYdLdhwwYvX7lyJZoza9YsL1dXV3t579690Zyw\nqdDZs2e9nLTJUHhszrlozJo1a7w8bdo0Lw8dGp+kyS3ATPp+AABkXG0Nwc3OuWPpx1WSbr5OxwMA\nADrBNRcVutR/zsb/SZtmZo+aWbmZlV+6dOlaPw4AAHSAq10QHDezUZKU/rO6uYHOueXOuQXOuQVJ\n+wUAAIDOd7WNiVZIekTS0+k/X2l5eEpTU5NyzxL8+c9/jsaMGzfOy4sWLfLyz372s2jO5cuXvdzY\n2BiNKSsr83K4SVLSRkXhZ4ebEPXp0yeaE9YMhM2M1q1bF83p27evl0eMGBGNOXjwoJfDBk4VFRXR\nnNzGREkbSQEAkNGW2w5/J+n/SZpmZofN7GtKLQQ+Y2a7JX06nQEAQDfVlrsMHm7mpaXNPA8AALoZ\nOhUCAABZ0j3vHWX48OHu/vvvz+bw2rkUX3OvrKz08ty5c6M5JSUlXj5//nw0Jvye4R0PJ06ciOaE\nNQIDBw5sdU64iVK42VHSsYWbPCX1NxgyZIiXJ02a5OVwsyNJ2rNnT/bxihUrdPLkybgJAgAA4gwB\nAAAQCwIAACAWBAAAQCwIAACArr4x0VUpKirS7Nmzs7mqqioa84tf/MLL48eP93JdXV00Z8qUKV6u\nqamJxjQ1NXn5wIEDLb4uxZsbhWOSiv9Gjhzp5bB4cefOndGcUaNGtfgeUvydtm/f7uWwOVN4vEnf\nDwCADM4QAAAAFgQAAIAFAQAAUJ5rCCSpoKAg+3jZsmXR62Hzn9raWi9PmDAhmrN69Wovnzp1KhoT\nXqcPGyAlzSkvL/fyLbfc4uWwoZAkXbx40cvDhg3zclIzptGjR3s59+8ow8zvKRR+n3BTJcnfwOmD\nDz6IXgcAIIMzBAAAgAUBAABgQQAAAJTnGoJLly5p7dq12bxr165oTHidPne8JH300UfRnNxr5VKq\n30GourrayyNGjPByWVlZ4vHmCu/979evXzQn3ITozTff9PKiRYuiOVeuXPFyUn+DGTNmeDmsKQjf\nQ5IaGhqyjxsbG6PXAQDI4AwBAABgQQAAAFgQAAAAsSAAAADKc1FhQUGBhg8f3uKY06dPt/j6gw8+\nGD0XNggqLS2NxoSb/4TFiWfOnInmVFRUeDncaCl8XZJKSkq8PG7cOC8nbTJ04sQJL4cFg5K0adMm\nL+cWDEpScXFxNGfatGnZx717570HFQCgG+EMAQAAYEEAAABYEAAAAOW5hqB3797eZj/h9XYpvi4f\nXnPfvXt3NKe+vt7LO3bsiMaETYbCpkiDBg2K5oRNksKNiaZMmRLNmThxopfr6uq8fODAgWhO2BTp\n+PHj0Ziamhovh5tAnTt3LpoDAEBbcYYAAACwIAAAACwIAACA8lxDIEnOuezj5557Lnr90Ucf9fLk\nyZO9vHr16mjOu+++6+WjR4+2+LmS9NRTT3k56T7+sM4g7AXQq1e8nlqzZo2Xw/v/w02VJGndunVe\nDnsMSHHNQPh9JkyYEM3JraVI2vwIAIAMzhAAAAAWBAAAgAUBAAAQCwIAAKA8FxX26dNHo0ePzuZ5\n8+ZFY8LGPWFjotz5GZ///Oe9HG52JMUNjV544QUvNzY2RnPCzwo3Krp48WI0Z+nSpV7esGGDl6dP\nnx7NCRsRVVdXR2Pmz5/v5b1793o5aUOkBQsWZB+//fbb0esAAGRwhgAAALAgAAAAbVgQmNk4M1tp\nZhVmts3MvpF+fpiZvW1mu9N/Du34wwUAAB2hLTUEDZK+5Zxbb2aDJK0zs7cl/a2kd5xzT5vZk5Ke\nlPREez68trY2em7mzJleDjc7yt0cKeO9997zctjER5JGjhzp5aFD/fXLkCFDojlhU6GwEVFpaWk0\nJ2xeNHXqVC+fOnUqmhPWTYTHKkkff/yxl2+99VYvh5sfSX79Qri5EwAAuVo9Q+CcO+acW59+fF7S\ndkljJD0kKdNq8DlJn+uogwQAAB2rXTUEZjZR0lxJayTd7Jw7ln6pStLN1/XIAABA3rR5QWBmAyW9\nJOmbzrlzua+5VGN918y8R82s3MzKL1y4cE0HCwAAOkabFgRm1kepxcBvnHMvp58+bmaj0q+PkhTf\nPC/JObfcObfAObcg6do+AADofK0WFVqq482vJG13zv0k56UVkh6R9HT6z1dae69Lly5p/fr12Zy0\nA9/27du9HO7ql7SouPfee71cVVXV6vueOXPGy0m7HY4fP97LYRHkzp07ozlhM6NwJ8OwyFCSCgsL\nvbx///5oTElJiZf37dvn5cOHD0dzpkyZkn0cFkgCAJCrLb8lbpf0VUlbzGxj+rnvKLUQ+IOZfU3S\nAUlf7phDBAAAHa3VBYFzbrWkuC9uytJmngcAAN0InQoBAEB+Nzfq16+f18xnxIgR0ZjwWn94bf/9\n99+P5uzatcvLSQ2Pwmv3YS4qKormhBsihZsmTZs2LZpz/vx5L8+ePdvLYe2CJIV3XyxZsiQaE25e\ndPbsWS+fOHEimpP790sNAQCgJZwhAAAALAgAAAALAgAAoDzXEDQ1NXm9B7Zt2xaNaWho8HJ4rTzc\n/EiSJk2a5OWDBw9GY8Jr++Hn1NXVRXPC9wlrHpI2Kgp7K3ziE5/wcmVlZTRn/vz5Xl67dm00Jqwh\n6Nu3r5cHDRoUzdmyZUv28eXLl6PX0TWZ2TBJv5c0UdJ+SV92zkXFJ2bWKCnzQz7onHswX8cIoOfh\nDAHQ9Typ1E6iZZLeSeckl51zc9L/sBgAcE1YEABdDzuJAsg77kUDup627iRaaGblkhokPe2c+/ek\nQWb2qKRHJamgoGB+ePtsSzpy/5GwZff1FLY8b0l4Oa417Tnu+vr6dr13Y2Njm8cOHTq0Xe997ty5\n1gflaGpqavPYI0eOtOu92/N33t7Lnf379++w9+7Tp0+bx7b3f99Jrfyb097/zdbV1Z10zpW0No4F\nAdAJzOwvkkYmvPTd3OCcc2bW3G+3Cc65I2Y2SdJfzWyLc25vOMg5t1zSckkqLi52t99+e5uPc/Hi\nxW0e295/Sd1yyy1tHtueX05S+34R9+vXr13vnbQfSXOS+oO0pKamps1jv/SlL7Xrvd966612jW/P\nL6jvfve7rQ/KUVBQ0OaxubVQbVFWVtbmsZs3b27Xe998c3Nr82s7Dinue9OSXr3ad3L/4MGDB9oy\nLu8Lgtx/aSxatCh6/dKlS14OC/eqq+NNFffs2ePlpIZH4Q8yXOmFDYSkuPAw3EAoLAaUpIsXL3o5\nbEQUFkBK0o4dO7yc9H+W8LPCBk5Jfy9DhgzJPm7vv6zRsZxzn27uNTPL7CR6m6SfSRpgZk86554O\nhp40s99Lmi9pgKR7JT3TUccMoGejhgDoelZI+ltJP5f0sqT/LelhM5uRGWBmQyU9JumMpMVKXTag\nsBDAVeOSAdD1PC3pDaUuKcxTaifRU5L+q5kVOuf+TtJ0ST9Uqsbgdknfk/Q/zcxcey6gA0AaZwiA\nLsY5d0rSjyT9zjn3aefcaUmH06/9XfrPD5XadvxTzrlZzrlfSDoraXgnHTaAbi6vZwiuXLniXft+\n/vnnozGPPfaYl8Nr7kmb9IR1B7nXzjOmT5/u5dWrV3t53bp10Zxww6MHHnjAy0lFNzt37vTybbfd\n5uWkzY3CqtiRI+Nas7A4KWykNG/evGjOgQNtqiNBD5d7l0FHVvYD6N44QwB0TUckjcvJY9PPJY4x\ns96SipW6tOBxzi13zi1wzi0IO1wCQAY1BEDXtFbSbDOrlNSk1F0E4Z0Jx5W63XCnpCGS9lE/AOBq\ncYYA6Joyv9gt/Y8kOTP7JzPL3E3wvlJnCQZKqpb0H/J7iAB6kryeIRgwYIC32c+SJUuiMatWrfJy\n2Ckt6bp9eN9+Uj1A0oZHuZKurYa9Cj766CMvJzVLCXsgrF+/3ssLFiyI5uzatcvLSZ22iouLvRzW\nRCR1OKP3QLe2UNJm59x9kmRmT0l6yDn3vZwx9ZLecM493hkHCKBn4QwB0DWNkXQoJx9OPxf6oplt\nNrMXzWxcwusA0CbUEADd16tK3ZpYa2aPKbUR0j3hoNy7DCRdeP3113eGYySNkHQyfPL111+/jofb\nJSR+zx7mRviOUjPfs73tiNsj7FZ7vca24nr8PCe0ZRALAqBravUug3S/goxfSvpfSW+Uu5dBc8ys\n3DkXX8/qYW6E73kjfEeJ79kRuGQAdE1rJZWZWamZ9ZX0FaVaGmel9zvIeFCSv8EFALRDXs8Q1NfX\new12knYkmzhxopc3btzo5fPnz0dz7rvvPi/nFi5mhEWF4emcpG1ewwZHYfFiUgOhsBnQTTfd5OWk\ngsFwh7Pa2tpWjyVscJR0f3nuZ7V3dyx0Ludcg5k9LulNSQWSnnXObTOzf5JU7pxbIenr6TsOGiSd\nVmr/AwC4KlwyALoo59xrkl4LnvtezuOnJD11nT6uxUsKPciN8D1vhO8o8T2vO/6zEUCmzqDHuxG+\n543wHSW+Z0dgQQAAAPJ7yaCxsVFnz57N5qQuq7NmzfJyVVWVl8ONjCRp5cqVXh4+PN7wLdwk6eLF\ni15OauJz+vRpL4cbDFVWVkZzwsZDYcOgsFGRJI0b598+nnS7yqZNm7w8duxYLyc1ScpttkQNAZKY\n2TJJP1WqTuGXzrmnO/mQOoSZ7Zd0XlKjpIaeUp1uZs9KekBStXPu1vRzwyT9XtJESfslfdk5F++q\n1o008z3/UdJ/kZT5F/N30pfZuqV0H5H/I+lmpTqVLnfO/TSfP09+SwA3KDMrkPRzSZ+VNEPSw2Y2\no3OPqkN9yjk3p6csBtJ+LWlZ8NyTkt5xzpVJeiedu7tfK/6ekvQv6Z/pnO68GEhrkPQt59wMSYsl\n/X36/495+3myIABuXAsl7XHOVTrn6iS9IOmhTj4mtINzbpVSd5jkekipJlVK//m5vB5UB2jme/Yo\nzrljzrn16cfnlbqNeIzy+PNkQQDcuNraHrkncJLeMrN16c6NPdnNzrlj6cdVSp2C7qkeT7fuftbM\nhnb2wVwvZjZR0lxJa5THn2deawicc6qrq8vm6urqaMyUKVO8fO7cOS+PHz8+mhPWGezfvz8as3Xr\nVi/Pnj3by7m1DRmLFi3ycthTYPv2uA9MuPlSOCepX0BYmzB69OhoTNirIKxvmDx5cjQnt24g7KEA\n3GDucM4dMbObJL1tZjvS/9XZoznnnJn11C2xn5H0z0ot9v5Z0o8l/edOPaLrwMwGSnpJ0jedc+dy\n69s6+ufJGQLgxtVqe+Sewjl3JP1ntaQ/KnW5pKc6nulimf4z/i+vHsA5d9w51+ica5L0C/WAn6mZ\n9VFqMfAb59zL6afz9vNkQQDcuFptj9wTmNkAMxuUeSzpXklbW57Vra2Q9Ej68SOSXunEY+kwQevu\nz6ub/0wtdSrgV5K2O+d+kvNS3n6erV4yMLNCSask9UuPf9E5930zK1WqCGm4pHWSvpouTALQDTTX\nHrmTD6sj3Czpj+lTr70l/dY590bnHtL1YWa/k3S3pBFmdljS9yU9LekPZvY1SQckfbnzjvD6aOZ7\n3m1mc5S6ZLBf0mOddoDXx+2Svippi5llevZ/R3n8ebalhqBW0j3OuQvp0xmrzex1Sf+g1C0fL5jZ\nv0n6mlLXdAB0E0ntkXsa51ylpNs6+zg6gnPu4WZeWprXA+lgzXzPX+X9QDqQc261pLghTkpefp6t\nLghcqnvQhXTsk/7HKbXv+n9MP/+cpH9UKwuCfv36qbS0NJvDwjhJ+uEPf+jlL3zhC15euDC+TBQW\n1H3wwQfRmHDDo7CgMfe4MoYO9YtWw6ZIuQWSGadOnfJy2CSpoqIimjN//nwv79u3LxoTNk4aNGiQ\nl4uLi6M5x44dyz5OalwEAEBGm2oIzKwgfQqjWtLbkvZKqnHONaSH9OTblQAA6PHatCBIV3LOUaoK\neaGkW9r6AWb2qJmVm1l52C4YAAB0De26y8A5VyNppaQlkoaYWeaSQ7O3KznnljvnFjjnFgwYMOCa\nDhYAAHSMttxlUCKp3jlXY2b9JX1G0o+UWhh8Sak7Ddp0K0SvXr00cODAbF6yZEk0pqioyMt79+71\nctjoR5LmzJnj5aQ6gw0bNnj5+PHjXq6trY3mhM2AwpqB8Dp+0vH99a9/9XJYyyDF3/Hy5cvRmJkz\nZ3o5PP7y8vJoTu4CrKGhIXodAICMttxlMErSc+mNUHpJ+oNz7k9mViHpBTP7H5I2qIdVfAIAcCNp\ny10Gm5XqqRw+X6ke0BkKAADQqRAAAIgFAQAAUJ53O6yvr9fRo0ezOWnnwvC5sKHO2LFjozknT570\nclLDoLCxT1IjotCZM2dafI9wZ0NJamxs9HLYNKmysjKaU1hY2GKW4qLBIUOGeHnq1KnRnFxJuywC\nAJDBGQIAAMCCAAAAsCAAAADKcw1BQ0ODd73/jjvuiMaE1+nDRj9r166N5qxevdrLSdfLv/71r3v5\nttv8zc+SNlr6y1/+4uXczYKkeOMiKd4AqaqqysszZsyI5mzb5u84u2jRomhM2IgorF9I+s653yms\nbQAAIBdnCAAAAAsCAADAggAAACjPNQR9+/bVuHHjsnndunXRmLBmIKwP2Lx5czQn7Dtwzz33RGP6\n9+/v5TfeeMPL8+fPj+aENQ5hPUDYp0CK+wxMmjTJy+fOnYvmjBo1ystJGxHNmjXLyzt37vRyWLsg\nSRcuXMg+Dvs5AACQizMEAACABQEAAGBBAAAAxIIAAAAoz0WFBQUF3qY8H3/8cTRm06ZNXg6L6e6/\n//5ozpo1a6LPCVVUVHh5w4YNXt69e3c0Z/bs2V4+cuSIl5MK+ebNm+flsEAwqUFQWPDXp0+faMyJ\nEye8fOedd3o5aaOl3PdJek8AADI4QwAAAFgQAAAAFgQAAEB5riGor6/3NulJ2pCnVy9/jRI24Bk8\neHA05+zZs16eMGFCNGbx4sVeLi0t9fKhQ4eiOdXV1V6eMmWKl/fs2RPNWblypZfHjBnT4ntK8Xfe\nvn17NCbc9Gn06NFeDhsiSfF3BACgOZwhAAAALAgAAAALAgAAoDzXEDQ1NXmb+8ycOTMaE/YQqK+v\n93JJSUk0J7xOv3Xr1mjM+vXrvRz2C5g2bVo0J9x8af/+/V5OqoHI7bMgxff/h30VJGnYsGFePnr0\naDQmfK53b/9HN3bs2GhOUVFR9nFYpwAAQC5+SwAAABYEAACABQEAABALAgAAoE7Y3Gj48OHZHBbt\nSdK3v/1tL4fFcy+99FI0Z8eOHV5OajIUCosMkwoEw8Y+4cZESRsGVVVVtTgmqTFRKNwQSZJXjCnF\nmzElNSHKLch0zrX6uQCAGxdnCAAAAAsCAADAggAAACjPNQRSqjlRRm1tbfR6WCMQblQUXkuXpNdf\nf93Lc+bMicYsW7bMy2GDoGPHjkVzws2Mwk2Hwo2XJGny5MlevnLlipcHDhwYzTlx4oSXcxsKZSxa\ntMjL4YZOW7ZsiebkNklK+rsGACCDMwQAAIAFAQAAaMeCwMwKzGyDmf0pnUvNbI2Z7TGz35tZfN8e\nAADoFtpTQ/ANSdslDU7nH0n6F+fcC2b2b5K+JumZlt7AzLy+AosXL47GfPjhh14ONyoqKyuL5vz4\nxz/2ckVFRTTmyJEjXp46daqXw42LJOnUqVNerqys9HLShkhhf4Ck4w2FYzZu3BiNCTd5qqmp8XLS\n32Xuhkb9+/dv9TgAADeuNp0hMLOxku6X9Mt0Nkn3SHoxPeQ5SZ/riAMEAAAdr62XDP5V0n+XlLlF\nYLikGudcpqXeYUljkiaa2aNmVm5m5ZcuXbqmgwUAAB2j1QWBmT0gqdo5F/cZbgPn3HLn3ALn3IKk\n2+kAAEDna0sNwe2SHjSzv5FUqFQNwU8lDTGz3umzBGMlHWnhPQAAQBfW6oLAOfeUpKckyczulvTf\nnHP/ycz+r6QvSXpB0iOSXmntvXr16uU15kkq/jtz5oyX77rrLi/PnTs3mhNuZjRz5sxozKRJk7wc\nNhkKi/akuIjw5MmTXj58+HA0Z+HChV4Oi/+SNjc6ffq0l5M2Wgo/O3yfpI2WcjdJ4nINAKAl19KH\n4AlJ/2Bme5SqKfjV9TkkAACQb+1qXeyce1fSu+nHlZIWtjQeAAB0D3QqBAAA+d3cqKmpSefPn8/m\npA15pk+f7uWqqiovv/rqq9GcsCnPgAEDojEFBQVeHj9+vJeTNk0KGxOVlJR4ubCwMJoTXuvPvY4v\nxRsmSfHGQ0k1BMOGDfOyc87LSY2Hco8ld1MpAABCnCEAAAAsCAAAAAsCAACgPNcQNDQ0eNflv/jF\nL0ZjduzY4eV58+Z5ee3atdGc1atXe3nWrFnRmCtXrng5t5ZBSr62/9Zbb3l5xIgRXq6rq4vmLFq0\nyMthfcDgwYMV2rRpk5dzezVkhDUPYd+EsGeCJH3qU5/KPn733Xej1wEAyOAMAQAAYEEAAABYEAAA\nALEgAAAAynNRYShpO+SJEyd6efPmzV6ePHlyNCcsNFyxYkU0prS01MvhZkdJzYyWLl3q5XADobDI\nUJIuXLjg5UGDBnk53FRJijds2rt3bzQmLBq87bbbvBw2RJL8gkYaEwEAWsIZAgAAwIIAAACwIAAA\nAMpzDUFjY6PXEChpQ55Dhw55edSoUV5euXJlNGfMmDFevuuuu6IxYcOgP/7xj15+8803ozm9evnr\npXCDoa3C33Z5AAAEeElEQVRbt0Zzbr31Vi+H1+6Li4ujOWFjonAjJilucPThhx96eeTIkdGc3LqC\nsDETAAC5OEMAAABYEAAAABYEAABALAgAAIA6oTGRmWUfnz59Onr9k5/8pJeXL1/u5SFDhkRz+vbt\n6+WwMFGSzp496+Xevf2vHhYMSnHzokuXLnk5LGZM+uw5c+Z4eePGjdGccEfH3B0hMw4cOODl++67\nz8v79++P5hw8eDD7mMZEAICWcIYAAACwIAAAACwIAACA8lxDUFRU5G3K8/7770djlixZ4uWysjIv\n/+lPf4rmVFRUePmZZ56JxowdO9bLR48e9XLShkLhdffq6movX758OZoT1jiEGy+VlJREc8INkY4d\nOxaNCeskchs8SXEDJ8nfsGnVqlXR6wAAZHCGAAAAsCAAAAAsCAAAgCRzzuXtw0aMGOHuv//+bB4+\nfHg0ZsWKFV6eP3++lydPnhzNCe/9b2hoiMbk9j+QpLq6ulbnhD0Ewt4FYV8CSRo/fryXw/4HSXUH\nVVVVXu7Tp080JtwIKuxLcM8990Rz9uzZk338/PPPq6qqyqJBAACIMwQAAEAsCAAAgFgQAAAAsSAA\nAADKc2OiwsJCTZ8+PZuTiufC4riTJ096+cSJE9GccePGebm4uDga8+yzz3o5LAj8wQ9+EM0JiwjD\n5j+VlZXRnLCIMDz+pCLOsFAy6TteuXLFy3fccYeXjx8/Hs0pKirKPu7Vi7UfAKB5/JYAAAAsCAAA\nAAsCAACgPDcmMrMTkg5IGiHpZCvDu4rudKxS88c7wTkX76wEAIDyvCDIfqhZuXNuQd4/+Cp0p2OV\nut/xAgC6Bi4ZAAAAFgQAAKDzFgTLO+lzr0Z3Olap+x0vAKAL6JQaAgAA0LVwyQAAAOR3QWBmy8xs\np5ntMbMn8/nZbWFmz5pZtZltzXlumJm9bWa7038O7cxjzDCzcWa20swqzGybmX0j/XyXPF4AQNeW\ntwWBmRVI+rmkz0qaIelhM5uRr89vo19LWhY896Skd5xzZZLeSeeuoEHSt5xzMyQtlvT36b/Prnq8\nAIAuLJ9nCBZK2uOcq3TO1Ul6QdJDefz8VjnnVkk6HTz9kKTn0o+fk/S5vB5UM5xzx5xz69OPz0va\nLmmMuujxAgC6tnwuCMZIOpSTD6ef6+puds4dSz+uknRzZx5MEjObKGmupDXqBscLAOh6KCpsB5e6\nJaNL3ZZhZgMlvSTpm865c7mvdcXjBQB0TflcEByRNC4nj00/19UdN7NRkpT+s7qTjyfLzPootRj4\njXPu5fTTXfZ4AQBdVz4XBGsllZlZqZn1lfQVSSvy+PlXa4WkR9KPH5H0SiceS5aZmaRfSdrunPtJ\nzktd8ngBAF1bvnc7/BtJ/yqpQNKzzrkf5u3D28DMfifpbqV2DDwu6fuS/l3SHySNV2qnxi8758LC\nw7wzszskvS9pi6Sm9NPfUaqOoMsdLwCga6NTIQAAoKgQAACwIAAAAGJBAAAAxIIAAACIBQEAABAL\nAgAAIBYEAABALAgAAICk/w+DLLeHEPoW7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f475f260d10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "('Failure at ', 's12345678901234e      ', 's1234567890102e       ')\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQgAAAD8CAYAAACLgjpEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAEyRJREFUeJzt3X+MXWWdx/H3p50Z0FqX/oBCfwACDUlBqDpBiewGBCoQ\nYnVj3OLGrbuaukaCJm42rJuA0axxs/7Y7GJUxAZcFVxXqzVWoXZNgESxQ0OBCmwLlnTG0tIirZRi\nO53v/nHPkGHmPtyHe87MPXf4vJJm7j3ne8957sz0k3vufeb5KiIwM2tmRqcHYGb15YAwsyQHhJkl\nOSDMLMkBYWZJDggzS3JAmFmSA8LMkhwQZpbU0+kBNDNjxozo6Wk9tPPOOy/reA8++GDZIZlNK8PD\nw4yMjKhVneo41bqvry9OOumklnWDg4NZx1u8eHHZIbVFavn9p47ff5v+9u7dy5EjR1r+gpa6xJB0\nhaTHJO2QdH2T/cdJ+l6x/z5Jp5c5n5lNrbYDQtJM4CvAlcAy4BpJy8aVfQj4Q0ScBXwZ+Nd2z2dm\nU6/MK4gLgB0R8UREHAHuAFaOq1kJ3Fbc/h/gUuW87jazWigTEIuAXWPuDxbbmtZExDBwAJjX7GCS\n1kgakDQwMjJSYlhmVpXafMwZETdHRH9E9M+YUZthmb2qlfmfOAQsGXN/cbGtaY2kHuDPgP0lzmlm\nU6hMQGwGlkp6g6Q+YBWwflzNemB1cfu9wP+GP9cz6xptT5SKiGFJ1wJ3AjOBtRGxTdJngIGIWA98\nE/gvSTuAZ2iEiJl1iVpOlOrt7Y1585q+l/kSAwMDWce74447suq+8IUvZNWZdbv9+/dz9OjRyZ0o\nZWbTmwPCzJIcEGaW5IAwsyQHhJklOSDMLMkBYWZJDggzS3JAmFlSLWdS5i45l+t3v/tdVt2yZePX\nu2nu8OHDZYZj1nFTsuScmU1vDggzS3JAmFmSA8LMkhwQZpZUZtn7JZJ+Kem3krZJ+niTmoslHZD0\nQPHvhnLDNbOpVKb13jDwyYjYImk2cL+kjRHx23F190TE1SXOY2Yd0vYriIjYHRFbitt/BB5h4rL3\nZtbFKnkPomip9ybgvia7L5S0VdLPJJ1TxfnMbGqU7u4t6XXAD4BPRMTBcbu3AKdFxHOSrgJ+BCxN\nHGcNsAZg5syZlTa1Pffcc7Pqtm3bllW3dGnTpzDB8PBwVl2Vqm5cVseZtjZ1yjbv7aURDt+JiB+O\n3x8RByPiueL2BqBX0vxmx3LjHLP6KfMphmgsa/9IRHwpUXPyaC9OSRcU53PjHLMuUeYS4+3AB4CH\nJD1QbPsUcCpARHyNRrOcj0oaBg4Dq9w4x6x7lGmccy/wshe8EXETcFO75zCzzvLFvpklOSDMLMkB\nYWZJDggzS3JAmFlS6ZmUk6XKGYGHDh3KqnvLW96SVbd169asuje+8Y0ta+r+qW/VMzOtu/gVhJkl\nOSDMLMkBYWZJDggzS3JAmFmSA8LMkhwQZpbkgDCzJAeEmSXVdiblyMjIlJ9z//68xa7WrFmTVbd5\n8+aWNf39/VnH6sT3w6z0KwhJOyU9VDTGGWiyX5L+Q9IOSQ9KenPZc5rZ1KjqFcQlEbEvse9KGitZ\nLwXeCny1+GpmNTcV70GsBL4VDb8GTpB0yhSc18xKqiIgArhL0v1Fb4vxFgG7xtwfxB24zLpCFZcY\nF0XEkKSTgI2SHo2Iu1/pQcY3zjGzziv9CiIihoqve4F1wAXjSoaAJWPuLy62jT+OG+eY1UzZzlqz\nis7eSJoFrAAeHle2Hvib4tOMtwEHImJ3mfOa2dQoe4mxAFhXrDrUA3w3In4u6e/hxeY5G4CrgB3A\n88DfljynmU0R1XHJs76+vpg/v2kLz66ycuXKljXXXXdd1rFWrFiRVXfs2LGsOnt127dvH0eOHGm5\nnqAv9s0syQFhZkkOCDNLckCYWZIDwsySHBBmluSAMLMkB4SZJTkgzCyptjMpTzzxxE4PY0osXLgw\nq27Tpk1Zdeeff35W3ZEjR7LqbHp6+umnPZPSzMpxQJhZkgPCzJIcEGaW5IAwsyQHhJkltR0Qks4u\nmuWM/jso6RPjai6WdGBMzQ3lh2xmU6XtJeci4jFgOYCkmTQWol3XpPSeiLi63fOYWedUdYlxKfB4\nRDxZ0fHMrAaqar23Crg9se9CSVuB3wP/EBHbmhWN74tRxxmek2FoaEIHgKbe//73Z9Xdc889WXWX\nX355Vt2BAwey6mx6qqJ5bx/wLuD7TXZvAU6LiPOB/wR+lDqO+2KY1U8V/xOvBLZExJ7xOyLiYEQ8\nV9zeAPRK6v7lqs1eJaoIiGtIXF5IOllF0wxJFxTn21/BOc1sCpR6D6LopnU58JEx28Y2zXkv8FFJ\nw8BhYFW8Wt5cMJsGSgVERBwC5o3b9rUxt28CbipzDjPrHL8baGZJDggzS3JAmFmSA8LMkrwm5TRz\nxhlnZNV9//vN5rVNdOGFF2bVeY3L7uI1Kc2sNAeEmSU5IMwsyQFhZkkOCDNLckCYWZIDwsySHBBm\nluSAMLOkWs6k7O3tjfnzvfDUZLruuuuy6s4555ysuhtuyOtokLvG5QsvvJBVZ+3Zt28fR48erWYm\npaS1kvZKenjMtrmSNkraXnydk3js6qJmu6TV+U/BzDot9xLjVuCKcduuBzZFxFJgU3H/JSTNBW4E\n3gpcANyYChIzq5+sgIiIu4Fnxm1eCdxW3L4NeHeTh74T2BgRz0TEH4CNTAwaM6upMm9SLoiI3cXt\np4AFTWoWAbvG3B8stplZF6ikcU5EhKRS73aObZzjvhhm9VDmf+IeSacAFF/3NqkZApaMub+42DaB\nG+eY1U+Z/4nrgdFPJVYDP25ScyewQtKc4s3JFcU2M+sCuR9z3g78Cjhb0qCkDwGfBy6XtB24rLiP\npH5JtwBExDPAZ4HNxb/PFNvMrAtkvQcREdckdl3apHYA+PCY+2uBtW2Nzsw6qpYzKXt6euL1r399\np4cxrZ122mlZdZ/73Oey6o4ePZpV94tf/CKr7tvf/nZWnbXn4MGDDA8Pe01KM2ufA8LMkhwQZpbk\ngDCzJAeEmSU5IMwsyQFhZkkOCDNLckCYWVIlf+49GUZGRjo9hGntueeeq7QuV39/f1Zd7kzK3JnA\nuXVSy8mF2XLP2dfXl1XXiQ7qfgVhZkkOCDNLckCYWZIDwsySHBBmltQyIBJNc/5N0qOSHpS0TtIJ\nicfulPSQpAckDVQ5cDObfDmvIG5lYi+LjcC5EXEe8H/AP73M4y+JiOURkff5lpnVRsuAaNY0JyLu\niojh4u6vaaxWbWbTTBXvQfwd8LPEvgDuknR/0ffCzLpIqZmUkv4ZGAa+kyi5KCKGJJ0EbJT0aPGK\npNmxXmycI4nh4eFmZVaR3O/vk08+Ockjae7UU0/NqhscHMyq+9Of/pRVd9ZZZ7Wsefzxx7OOlTuT\n8rLLLsuq+8lPfpJVlzszM0fbryAkfRC4GvjrSHwnImKo+LoXWEejgW9TYxvnVDnd1cza11ZASLoC\n+EfgXRHxfKJmlqTZo7dpNM15uFmtmdVTzseczZrm3ATMpnHZ8ICkrxW1CyVtKB66ALhX0lbgN8BP\nI+Lnk/IszGxStHwPItE055uJ2t8DVxW3nwDOLzU6M+soz6Q0syQHhJklOSDMLMkBYWZJDggzS6rl\nmpQ9PT3MmzevZd3Q0FDW8V772tdm1eV2qD527FhW3YwZrfM3pwbyx5bruOOOy6q77777Kj1vrtNP\nPz2rbs6cOVl1W7Zsqey8O3fuzDrW8ccfn1WXu9bkmWeemVWXM2v0+eebTl+awK8gzCzJAWFmSQ4I\nM0tyQJhZkgPCzJIcEGaW5IAwsyQHhJklOSDMLKmWMyl7e3tZuHBhy7pdu3ZlHS93bcBcc+fOzao7\n4YSm7UJeInc2aK7cWZ65XbufeuqpMsNpW9Vdu08++eSsun379rWsmT17dtaxcmfw7tmzJ6vuNa95\nTVZd7u9AjnYb53xa0lCxmtQDkq5KPPYKSY9J2iHp+spGbWZTot3GOQBfLhriLI+IDeN3SpoJfAW4\nElgGXCNpWZnBmtnUaqtxTqYLgB0R8UREHAHuAFa2cRwz65Ayb1JeW/TmXCup2Z/ULQLGvkkwWGwz\nsy7RbkB8FTgTWA7sBr5YdiCS1kgakDRQ9Z82m1l72gqIiNgTEcciYgT4Bs0b4gwBS8bcX1xsSx3z\nxcY5vb297QzLzCrWbuOcU8bcfQ/NG+JsBpZKeoOkPmAVsL6d85lZZ7ScB1E0zrkYmC9pELgRuFjS\nchrNeXcCHylqFwK3RMRVETEs6VrgTmAmsDYitk3KszCzSTFpjXOK+xuACR+Bmll3qOVMyojI6j6d\n2wH6wIEDWXWzZs3KqluyZEnrIvI6aOfOaMyVu8ZlzoxBgEWLOvPB08jISKXHy31f64UXXmhZk7ue\nZ25dzjkhf13SKr93/lsMM0tyQJhZkgPCzJIcEGaW5IAwsyQHhJklOSDMLMkBYWZJtZwolSu3cWtu\nc9TciVIzZ87MqsuZKFW13CXYcscmqdLj9fTk/crlHi/3Z5E7ySjnd+DQoUOVHQvyf2Y5TXkh77nm\nntOvIMwsyQFhZkkOCDNLckCYWZIDwsySHBBmlpSzotRa4Gpgb0ScW2z7HnB2UXIC8GxELG/y2J3A\nH4FjwHBE9Fc0bjObAjkfSt8K3AR8a3RDRPzV6G1JXwRebkWWSyIib3USM6uVnCXn7pZ0erN9asyi\neR/wjmqHZWZ1UHYm5Z8DeyJie2J/AHdJCuDrEXFz6kCS1gBrAPr6+rKWTstd0it3GbbcWX65swv7\n+vqy6jqh6ua4+/fvz6pbsGBBVl3VqvzZ5i4Rl7vMXe6s0apnl+YoGxDXALe/zP6LImJI0knARkmP\nFq38JijC42aAWbNmVduO28za0vanGJJ6gL8EvpeqiYih4uteYB3NG+yYWU2V+ZjzMuDRiBhstlPS\nLEmzR28DK2jeYMfMaqplQBSNc34FnC1pUNKHil2rGHd5IWmhpNE+GAuAeyVtBX4D/DQifl7d0M1s\nsrXbOIeI+GCTbS82zomIJ4DzS47PzDrIMynNLMkBYWZJDggzS3JAmFlSLdeklJQ1+zF3hmTujLbc\nGWi5550OcmcN5jZInj9/flZdlbMBodpZrceOHcuqq3rt0tymvDnPNXc28KvnN93MXjEHhJklOSDM\nLMkBYWZJDggzS3JAmFmSA8LMkhwQZpbkgDCzpFrOpKza8ccfn1WXO/MtexZaxozL3GPlrg1ZtWef\nfTarLreDeqeeR9UzM3Pk/mxzZ0hW+XuXK2fBmCWSfinpt5K2Sfp4sX2upI2Sthdf5yQev7qo2S5p\ndWUjN7NJlxM1w8AnI2IZ8DbgY5KWAdcDmyJiKbCpuP8SkuYCNwJvpbEe5Y2pIDGz+mkZEBGxOyK2\nFLf/CDwCLAJWArcVZbcB727y8HcCGyPimYj4A7ARuKKKgZvZ5HtFFytFA503AfcBCyJid7HrKRpr\nUI63CNg15v5gsc3MukB2QEh6HfAD4BMRcXDsvmi881Tq3SdJayQNSBo4evRomUOZWUWyAkJSL41w\n+E5E/LDYvEfSKcX+U4C9TR46BCwZc39xsW2CiLg5Ivojoj93/QYzm1w5n2II+CbwSER8acyu9cDo\npxKrgR83efidwApJc4o3J1cU28ysC+S8gng78AHgHZIeKP5dBXweuFzSdhpNdD4PIKlf0i0AEfEM\n8Flgc/HvM8U2M+sCOX0x7gVSMzQubVI/AHx4zP21wNp2B2hmndPVMylzZ6DlzqSsuuN1jtyxHT58\nuLJzQv6svIMHD7YuIv9nkbv+Ym437lxVzlitelZm1d+TnDqvSWlmpTkgzCzJAWFmSQ4IM0tyQJhZ\nkgPCzJIcEGaW5IAwsyQHhJklqVNrBL4cSU8DT47bPB/Y14HhVMnPoT6mw/Mo8xxOi4gTWxXVMiCa\nkTQQEf2dHkcZfg71MR2ex1Q8B19imFmSA8LMkropIG7u9AAq4OdQH9PheUz6c+ia9yDMbOp10ysI\nM5titQ8ISVdIekzSDkkTmvN0C0k7JT1ULNk30Onx5JC0VtJeSQ+P2ZbVUa1OEs/j05KGxi2jWFtl\nO9y1q9YBIWkm8BXgSmAZcE3R1atbXRIRy7vo47VbmdjoqGVHtRq6leYNm75c/DyWR8SGKR7TK9V2\nh7syah0QNNr17YiIJyLiCHAHjY5eNgUi4m5g/CLDOR3VaiXxPLpKyQ53bat7QEynzlwB3CXpfklr\nOj2YEnI6qnWLayU9WFyC1P5SaVQbHe7aVveAmE4uiog307hc+pikv+j0gMqqoqNaB30VOBNYDuwG\nvtjZ4eSZ7A5349U9ILI7c9VdRAwVX/cC62hcPnWjnI5qtRcReyLiWESMAN+gC34eJTrcta3uAbEZ\nWCrpDZL6gFU0Onp1FUmzJM0evU2jw9jDL/+o2srpqFZ7o/+pCu+h5j+Pkh3u2j9v3SdKFR8//Tsw\nE1gbEf/S4SG9YpLOoPGqARq9SL7bDc9D0u3AxTT+anAPcCPwI+C/gVNp/MXt++reLS3xPC6mcXkR\nwE7gI2Ou5WtH0kXAPcBDwGgTkk/ReB9i0n4etQ8IM+ucul9imFkHOSDMLMkBYWZJDggzS3JAmFmS\nA8LMkhwQZpbkgDCzpP8HM8087bejV0cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f47866bb150>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS!!!!!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    new_saver = tf.train.import_meta_graph('my-model.meta')\n",
    "    new_saver.restore(sess, tf.train.latest_checkpoint('./'))\n",
    "    \n",
    "    a = 12345678901234\n",
    "    q = 's'+ str(a) + 'e' + ' '*(MAXLEN-len(str(a))-2)\n",
    "    batchX = np.reshape(ctable.encode(q, MAXLEN),(1,MAXLEN,C))\n",
    "    \n",
    "    graph = tf.get_default_graph()\n",
    "    x_batch = graph.get_tensor_by_name(\"x_batch:0\")\n",
    "    pred = graph.get_tensor_by_name(\"pred:0\")\n",
    "    attn = graph.get_tensor_by_name(\"attn:0\")\n",
    "    wc = graph.get_tensor_by_name(\"wc:0\")\n",
    "    bc = graph.get_tensor_by_name(\"bc:0\")\n",
    "    \n",
    "    Wc, Bc = sess.run(\n",
    "                [wc, bc]\n",
    "            )\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(Wc)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(Bc)\n",
    "    plt.gcf().set_size_inches(10, 4)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    for i in range(1):\n",
    "        if i%1== 0:\n",
    "            print (i)\n",
    "        batchP, batchA = sess.run(\n",
    "                [pred,attn] ,\n",
    "                feed_dict={\n",
    "                    x_batch: batchX\n",
    "                })\n",
    "        #a += 1\n",
    "        b = 's'+str(a)+'e'+(' ')*(MAXLEN-len(str(a))-2)\n",
    "        q = ctable.decode(batchP[0])\n",
    "        if b!=q:\n",
    "            print ('Failure at ',b,q)\n",
    "            q = b[::-1]\n",
    "        else:\n",
    "            q = q[::-1]\n",
    "        batchX = np.reshape(ctable.encode(' '*(MAXLEN-len(str(a))-2) + 's'+ str(a) + 'e', MAXLEN),(1,MAXLEN,C))\n",
    "        \n",
    "        plt.subplot(1, 1, 1)\n",
    "        plt.imshow(batchA[0])\n",
    "        plt.gcf().set_size_inches(10, 4)\n",
    "        plt.show()\n",
    "    \n",
    "    print ('SUCCESS!!!!!')\n",
    "    \n",
    "    '''\n",
    "    w = [''.join(ctable.indices_char[i] for i in batchY[j]) for j in range(x_train.shape[0])]\n",
    "    q = [ctable.decode(batchP[i]) for i in range(x_train.shape[0])]\n",
    "    correct_examples = 0\n",
    "    for i,j in zip(w,q):\n",
    "        if (i==j):\n",
    "            correct_examples += 1\n",
    "        else :\n",
    "            print (i,j)\n",
    "    \n",
    "    print ('Validation accuracy', float(correct_examples)/x_train.shape[0])\n",
    "    '''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
